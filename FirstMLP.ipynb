{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d580aa1-7400-413c-8625-5a4ab8c979fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.30666e+05 0.00000e+00 6.76000e-01 ... 7.15000e-01 8.79170e+01\n",
      "  4.00000e+00]\n",
      " [1.49610e+05 0.00000e+00 4.20000e-01 ... 2.67000e-01 7.74890e+01\n",
      "  4.00000e+00]\n",
      " [2.10826e+05 0.00000e+00 4.38000e-01 ... 1.20000e-01 7.63320e+01\n",
      "  4.00000e+00]\n",
      " ...\n",
      " [2.71466e+05 0.00000e+00 6.29000e-01 ... 7.43000e-01 1.32378e+02\n",
      "  4.00000e+00]\n",
      " [2.83893e+05 0.00000e+00 5.87000e-01 ... 4.13000e-01 1.35960e+02\n",
      "  4.00000e+00]\n",
      " [2.41826e+05 0.00000e+00 5.26000e-01 ... 7.08000e-01 7.91980e+01\n",
      "  4.00000e+00]] [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.dataset as ds\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor, Model\n",
    "import mindspore.dataset.vision as vision\n",
    "import mindspore.dataset.transforms as transforms\n",
    "from mindspore.train.callback import LossMonitor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: 加载并处理数据集\n",
    "df = pd.read_csv(\"processed_dataset.csv\")\n",
    "\n",
    "# 将 'explicit' 列转换为布尔值\n",
    "df['explicit'] = df['explicit'].astype(int)\n",
    "\n",
    "# 将 'track_genre' 进行 One-Hot 编码\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "track_genre_encoded = encoder.fit_transform(df[['track_genre']])\n",
    "\n",
    "# 将编码后的数据添加回数据框并移除原始的 'track_genre' 列\n",
    "encoded_columns = encoder.get_feature_names_out(['track_genre'])\n",
    "df_encoded = pd.DataFrame(track_genre_encoded, columns=encoded_columns)\n",
    "df = pd.concat([df.drop(columns=['track_genre']), df_encoded], axis=1)\n",
    "\n",
    "# 提取特征和标签\n",
    "X = df.drop(columns=encoded_columns).astype(np.float32).values\n",
    "y = track_genre_encoded\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92feafcd-6179-49ca-b5de-520eae682ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 21.395741  [  0/2850]\n",
      "loss: 4.753157  [100/2850]\n",
      "loss: 4.756130  [200/2850]\n",
      "loss: 4.735509  [300/2850]\n",
      "loss: 4.757111  [400/2850]\n",
      "loss: 4.730098  [500/2850]\n",
      "loss: 4.748776  [600/2850]\n",
      "loss: 4.732485  [700/2850]\n",
      "loss: 4.742818  [800/2850]\n",
      "loss: 4.789379  [900/2850]\n",
      "loss: 4.735844  [1000/2850]\n",
      "loss: 4.731573  [1100/2850]\n",
      "loss: 4.727586  [1200/2850]\n",
      "loss: 4.745066  [1300/2850]\n",
      "loss: 4.736310  [1400/2850]\n",
      "loss: 4.723031  [1500/2850]\n",
      "loss: 4.751857  [1600/2850]\n",
      "loss: 4.781862  [1700/2850]\n",
      "loss: 4.738662  [1800/2850]\n",
      "loss: 4.735434  [1900/2850]\n",
      "loss: 4.732615  [2000/2850]\n",
      "loss: 4.750091  [2100/2850]\n",
      "loss: 4.730134  [2200/2850]\n",
      "loss: 4.740123  [2300/2850]\n",
      "loss: 4.745975  [2400/2850]\n",
      "loss: 4.762704  [2500/2850]\n",
      "loss: 4.757313  [2600/2850]\n",
      "loss: 4.740844  [2700/2850]\n",
      "loss: 4.719240  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 0.9%, Avg loss: 4.741728 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 4.739935  [  0/2850]\n",
      "loss: 4.765605  [100/2850]\n",
      "loss: 4.746161  [200/2850]\n",
      "loss: 4.735233  [300/2850]\n",
      "loss: 4.733174  [400/2850]\n",
      "loss: 4.722392  [500/2850]\n",
      "loss: 4.724863  [600/2850]\n",
      "loss: 4.714701  [700/2850]\n",
      "loss: 4.734244  [800/2850]\n",
      "loss: 4.745491  [900/2850]\n",
      "loss: 4.750159  [1000/2850]\n",
      "loss: 4.745383  [1100/2850]\n",
      "loss: 4.743710  [1200/2850]\n",
      "loss: 4.730085  [1300/2850]\n",
      "loss: 4.752493  [1400/2850]\n",
      "loss: 4.741899  [1500/2850]\n",
      "loss: 4.712494  [1600/2850]\n",
      "loss: 4.709411  [1700/2850]\n",
      "loss: 4.726057  [1800/2850]\n",
      "loss: 4.730362  [1900/2850]\n",
      "loss: 4.737833  [2000/2850]\n",
      "loss: 4.741834  [2100/2850]\n",
      "loss: 4.738655  [2200/2850]\n",
      "loss: 4.748180  [2300/2850]\n",
      "loss: 4.724522  [2400/2850]\n",
      "loss: 4.751142  [2500/2850]\n",
      "loss: 4.736306  [2600/2850]\n",
      "loss: 4.728499  [2700/2850]\n",
      "loss: 4.745001  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 0.9%, Avg loss: 4.740118 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 4.745462  [  0/2850]\n",
      "loss: 4.749394  [100/2850]\n",
      "loss: 4.734574  [200/2850]\n",
      "loss: 4.750493  [300/2850]\n",
      "loss: 4.722170  [400/2850]\n",
      "loss: 4.734735  [500/2850]\n",
      "loss: 4.728930  [600/2850]\n",
      "loss: 4.739014  [700/2850]\n",
      "loss: 4.741364  [800/2850]\n",
      "loss: 4.748400  [900/2850]\n",
      "loss: 4.772065  [1000/2850]\n",
      "loss: 4.744055  [1100/2850]\n",
      "loss: 4.717076  [1200/2850]\n",
      "loss: 4.758595  [1300/2850]\n",
      "loss: 4.773127  [1400/2850]\n",
      "loss: 4.745931  [1500/2850]\n",
      "loss: 4.758247  [1600/2850]\n",
      "loss: 4.715484  [1700/2850]\n",
      "loss: 4.731183  [1800/2850]\n",
      "loss: 4.741284  [1900/2850]\n",
      "loss: 4.736753  [2000/2850]\n",
      "loss: 4.738601  [2100/2850]\n",
      "loss: 4.711524  [2200/2850]\n",
      "loss: 4.724730  [2300/2850]\n",
      "loss: 4.739608  [2400/2850]\n",
      "loss: 4.744214  [2500/2850]\n",
      "loss: 4.778728  [2600/2850]\n",
      "loss: 4.765507  [2700/2850]\n",
      "loss: 4.716275  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 0.9%, Avg loss: 4.740710 \n",
      "\n",
      "Done!\n",
      "Predicted class probabilities: [[-9.83400084e-03  4.80060913e-02  1.45770088e-02  3.97890024e-02\n",
      "  -1.34478599e-01 -8.12019259e-02 -8.35659429e-02  8.22734740e-03\n",
      "  -5.11589684e-02 -1.32241687e-02  1.71257136e-03 -9.47498158e-02\n",
      "  -1.28931133e-04  2.72808168e-02 -1.03694305e-01 -1.08270764e-01\n",
      "  -6.48103803e-02  6.27931356e-02  5.21612167e-02  1.86678320e-02\n",
      "   4.39398549e-02 -7.66460178e-03 -1.72900911e-02  2.99997553e-02\n",
      "   1.36553928e-01  2.49614507e-01  6.33512288e-02  3.21472771e-02\n",
      "  -4.41285297e-02 -1.09827751e-03 -3.22114155e-02  2.20955443e-02\n",
      "  -1.75496172e-02  1.90853942e-02 -1.23285048e-01 -5.82334213e-02\n",
      "   8.77037272e-02 -1.26248281e-02  2.51188856e-02 -4.25983146e-02\n",
      "  -1.97180822e-01  7.58342519e-02 -7.76421204e-02 -5.88710606e-02\n",
      "   5.79601079e-02 -7.31963757e-03  8.72897450e-04  1.55676110e-03\n",
      "  -1.38127923e-01 -6.59684092e-02 -1.84434071e-01 -2.21401513e-01\n",
      "   6.37699217e-02 -4.25421782e-02 -1.99557785e-02 -2.80045830e-02\n",
      "  -1.69507727e-01 -9.21975938e-04  3.44530754e-02 -1.44062936e-01\n",
      "   7.79378489e-02  6.51473850e-02  3.40430439e-02 -6.52378350e-02\n",
      "  -6.16209134e-02 -1.79891244e-01 -7.78328553e-02 -1.47765234e-01\n",
      "  -4.48043682e-02 -1.20728411e-01 -1.76092505e-01 -3.60477939e-02\n",
      "   1.37629211e-02 -1.75328925e-01 -2.06651613e-01 -3.39609347e-02\n",
      "  -3.65385562e-02 -4.27906439e-02  7.35359499e-05  7.56494924e-02\n",
      "  -2.37580478e-01 -6.92746937e-02 -1.88561156e-02 -1.31291687e-01\n",
      "   7.59598836e-02 -3.07835080e-02 -1.37276286e-02  8.59444886e-02\n",
      "   6.68530911e-02  1.06846407e-01 -2.45341003e-01  1.83470733e-03\n",
      "   4.70052958e-02 -1.08342431e-01  8.38631839e-02 -7.82866180e-02\n",
      "  -5.20478375e-02 -8.65429640e-04 -7.93600753e-02 -4.00991887e-02\n",
      "  -1.98492542e-01 -2.91952491e-02  6.27962723e-02 -1.48677409e-01\n",
      "   3.70798148e-02  1.73783019e-01 -5.65563701e-02 -4.76191565e-02\n",
      "   1.43173903e-01 -5.22738770e-02 -3.57262611e-01 -9.79314223e-02\n",
      "  -4.63265330e-02 -5.11890650e-02]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: 定义一个简单的 MLP 分类器\n",
    "class MLPClassifier(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # 输入层到第一层隐藏层的全连接层\n",
    "        self.fc1 = nn.Dense(input_size, hidden_sizes[0], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # 第一层隐藏层到第二层隐藏层的全连接层\n",
    "        self.fc2 = nn.Dense(hidden_sizes[0], hidden_sizes[1], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # 第二层隐藏层到输出层的全连接层\n",
    "        self.fc3 = nn.Dense(hidden_sizes[1], output_size, weight_init=\"normal\")\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 数据通过输入层进入第一层隐藏层\n",
    "        x = self.fc1(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu1(x)\n",
    "        # 数据通过第一层隐藏层进入第二层隐藏层\n",
    "        x = self.fc2(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu2(x)\n",
    "        # 数据通过第二层隐藏层进入输出层\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: 创建模型实例\n",
    "input_size = X.shape[1]  # 输入层的特征数量\n",
    "hidden_sizes = [100, 50]  # 隐藏层中的神经元数量\n",
    "output_size = y.shape[1]  # 输出层的神经元数量（类别数量）\n",
    "model = MLPClassifier(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Step 4: 定义超参数、损失函数和优化器\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction='mean')  # 损失函数\n",
    "optimizer = nn.Adam(params=model.trainable_params(), learning_rate=learning_rate)  # 优化器\n",
    "\n",
    "# Step 5: 划分训练集和测试集\n",
    "# 将数据集划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# train_dataset = ds.NumpySlicesDataset(data=(X_train, y_train), column_names=['features', 'labels'], shuffle=True).batch(batch_size)\n",
    "# test_dataset = ds.NumpySlicesDataset(data=(X_test, y_test), column_names=['features', 'labels'], shuffle=False).batch(batch_size)\n",
    "train_dataset = ds.NumpySlicesDataset(data=(X_train, y_train.astype(np.float32)), column_names=['features', 'labels'], shuffle=True).batch(batch_size)\n",
    "test_dataset = ds.NumpySlicesDataset(data=(X_test, y_test.astype(np.float32)), column_names=['features', 'labels'], shuffle=False).batch(batch_size)\n",
    "\n",
    "\n",
    "# Step 6: 定义训练和测试函数\n",
    "def forward_fn(data, label):\n",
    "    logits = model(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss, logits\n",
    "\n",
    "grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "def train_step(data, label):\n",
    "    (loss, _), grads = grad_fn(data, label)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def train_loop(model, dataset, loss_fn):\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "        if batch % 100 == 0:\n",
    "            loss_value = loss.asnumpy()\n",
    "            print(f\"loss: {loss_value:>7f}  [{batch:>3d}/{size:>3d}]\")\n",
    "\n",
    "def test_loop(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label.argmax(1)).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Step 7: 开始训练与测试\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(model, train_dataset, loss_fn)\n",
    "    test_loop(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Step 8: 使用模型进行推理\n",
    "# 创建一个形状为 (1, input_size) 的全为 1 的张量作为输入\n",
    "X_infer = Tensor(np.ones((1, input_size)), ms.float32)\n",
    "# 使用模型进行前向传播，获取输出 logits\n",
    "y_pred = model(X_infer)\n",
    "# 打印预测结果\n",
    "print(f\"Predicted class probabilities: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e330933-e39c-413e-b9fb-855e51afee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.759555  [  0/2850]\n",
      "loss: 3.973511  [100/2850]\n",
      "loss: 4.104115  [200/2850]\n",
      "loss: 3.984339  [300/2850]\n",
      "loss: 3.739089  [400/2850]\n",
      "loss: 4.028446  [500/2850]\n",
      "loss: 3.910594  [600/2850]\n",
      "loss: 3.568999  [700/2850]\n",
      "loss: 4.055288  [800/2850]\n",
      "loss: 4.027919  [900/2850]\n",
      "loss: 3.271993  [1000/2850]\n",
      "loss: 3.691371  [1100/2850]\n",
      "loss: 3.728622  [1200/2850]\n",
      "loss: 3.668241  [1300/2850]\n",
      "loss: 3.562929  [1400/2850]\n",
      "loss: 3.645488  [1500/2850]\n",
      "loss: 3.602694  [1600/2850]\n",
      "loss: 3.399765  [1700/2850]\n",
      "loss: 3.629832  [1800/2850]\n",
      "loss: 3.382724  [1900/2850]\n",
      "loss: 3.915841  [2000/2850]\n",
      "loss: 3.472497  [2100/2850]\n",
      "loss: 3.343732  [2200/2850]\n",
      "loss: 3.776298  [2300/2850]\n",
      "loss: 3.547395  [2400/2850]\n",
      "loss: 3.523124  [2500/2850]\n",
      "loss: 3.872051  [2600/2850]\n",
      "loss: 3.494771  [2700/2850]\n",
      "loss: 3.804004  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 14.2%, Avg loss: 3.597808 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.848297  [  0/2850]\n",
      "loss: 3.436370  [100/2850]\n",
      "loss: 3.921732  [200/2850]\n",
      "loss: 3.335642  [300/2850]\n",
      "loss: 3.695893  [400/2850]\n",
      "loss: 3.411799  [500/2850]\n",
      "loss: 4.268157  [600/2850]\n",
      "loss: 3.337048  [700/2850]\n",
      "loss: 3.657945  [800/2850]\n",
      "loss: 3.884098  [900/2850]\n",
      "loss: 3.566390  [1000/2850]\n",
      "loss: 3.749904  [1100/2850]\n",
      "loss: 3.437878  [1200/2850]\n",
      "loss: 3.503923  [1300/2850]\n",
      "loss: 3.000466  [1400/2850]\n",
      "loss: 3.773201  [1500/2850]\n",
      "loss: 3.822303  [1600/2850]\n",
      "loss: 3.391673  [1700/2850]\n",
      "loss: 3.570907  [1800/2850]\n",
      "loss: 3.247464  [1900/2850]\n",
      "loss: 3.303672  [2000/2850]\n",
      "loss: 3.538671  [2100/2850]\n",
      "loss: 3.720408  [2200/2850]\n",
      "loss: 3.833960  [2300/2850]\n",
      "loss: 3.819288  [2400/2850]\n",
      "loss: 3.752599  [2500/2850]\n",
      "loss: 3.884340  [2600/2850]\n",
      "loss: 3.713313  [2700/2850]\n",
      "loss: 3.549881  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 14.8%, Avg loss: 3.556743 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.778971  [  0/2850]\n",
      "loss: 3.095756  [100/2850]\n",
      "loss: 3.311669  [200/2850]\n",
      "loss: 3.490460  [300/2850]\n",
      "loss: 3.670039  [400/2850]\n",
      "loss: 3.589532  [500/2850]\n",
      "loss: 3.444339  [600/2850]\n",
      "loss: 3.982913  [700/2850]\n",
      "loss: 3.681938  [800/2850]\n",
      "loss: 3.691316  [900/2850]\n",
      "loss: 3.769349  [1000/2850]\n",
      "loss: 3.472653  [1100/2850]\n",
      "loss: 3.607654  [1200/2850]\n",
      "loss: 3.609537  [1300/2850]\n",
      "loss: 3.717404  [1400/2850]\n",
      "loss: 3.161262  [1500/2850]\n",
      "loss: 3.119972  [1600/2850]\n",
      "loss: 3.339779  [1700/2850]\n",
      "loss: 3.876855  [1800/2850]\n",
      "loss: 3.402091  [1900/2850]\n",
      "loss: 3.875807  [2000/2850]\n",
      "loss: 3.444772  [2100/2850]\n",
      "loss: 3.099284  [2200/2850]\n",
      "loss: 3.607175  [2300/2850]\n",
      "loss: 3.290434  [2400/2850]\n",
      "loss: 3.434070  [2500/2850]\n",
      "loss: 3.113840  [2600/2850]\n",
      "loss: 3.633421  [2700/2850]\n",
      "loss: 3.614572  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.5%, Avg loss: 3.541384 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.796240  [  0/2850]\n",
      "loss: 3.799676  [100/2850]\n",
      "loss: 3.524361  [200/2850]\n",
      "loss: 3.388561  [300/2850]\n",
      "loss: 3.780354  [400/2850]\n",
      "loss: 3.717815  [500/2850]\n",
      "loss: 3.422153  [600/2850]\n",
      "loss: 3.355878  [700/2850]\n",
      "loss: 4.047108  [800/2850]\n",
      "loss: 3.213447  [900/2850]\n",
      "loss: 3.716088  [1000/2850]\n",
      "loss: 3.533926  [1100/2850]\n",
      "loss: 3.281405  [1200/2850]\n",
      "loss: 3.529003  [1300/2850]\n",
      "loss: 3.640796  [1400/2850]\n",
      "loss: 4.154770  [1500/2850]\n",
      "loss: 3.268823  [1600/2850]\n",
      "loss: 3.346405  [1700/2850]\n",
      "loss: 3.182468  [1800/2850]\n",
      "loss: 3.746866  [1900/2850]\n",
      "loss: 3.443113  [2000/2850]\n",
      "loss: 3.424860  [2100/2850]\n",
      "loss: 3.701110  [2200/2850]\n",
      "loss: 3.457286  [2300/2850]\n",
      "loss: 3.516098  [2400/2850]\n",
      "loss: 3.863532  [2500/2850]\n",
      "loss: 3.664087  [2600/2850]\n",
      "loss: 3.473380  [2700/2850]\n",
      "loss: 3.204437  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.3%, Avg loss: 3.536380 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.739253  [  0/2850]\n",
      "loss: 3.942691  [100/2850]\n",
      "loss: 3.771334  [200/2850]\n",
      "loss: 3.521712  [300/2850]\n",
      "loss: 3.491529  [400/2850]\n",
      "loss: 3.715957  [500/2850]\n",
      "loss: 3.555305  [600/2850]\n",
      "loss: 3.581403  [700/2850]\n",
      "loss: 3.540645  [800/2850]\n",
      "loss: 3.544513  [900/2850]\n",
      "loss: 3.344059  [1000/2850]\n",
      "loss: 3.363373  [1100/2850]\n",
      "loss: 3.879249  [1200/2850]\n",
      "loss: 3.787454  [1300/2850]\n",
      "loss: 3.320526  [1400/2850]\n",
      "loss: 3.642393  [1500/2850]\n",
      "loss: 3.725110  [1600/2850]\n",
      "loss: 3.717128  [1700/2850]\n",
      "loss: 3.545545  [1800/2850]\n",
      "loss: 3.747394  [1900/2850]\n",
      "loss: 3.315616  [2000/2850]\n",
      "loss: 3.666040  [2100/2850]\n",
      "loss: 3.228564  [2200/2850]\n",
      "loss: 3.547933  [2300/2850]\n",
      "loss: 3.407700  [2400/2850]\n",
      "loss: 3.921711  [2500/2850]\n",
      "loss: 3.170731  [2600/2850]\n",
      "loss: 3.705766  [2700/2850]\n",
      "loss: 3.342475  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.6%, Avg loss: 3.530736 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.703645  [  0/2850]\n",
      "loss: 3.473960  [100/2850]\n",
      "loss: 3.518305  [200/2850]\n",
      "loss: 3.452183  [300/2850]\n",
      "loss: 4.256311  [400/2850]\n",
      "loss: 3.441944  [500/2850]\n",
      "loss: 3.546223  [600/2850]\n",
      "loss: 3.678787  [700/2850]\n",
      "loss: 3.298096  [800/2850]\n",
      "loss: 3.352775  [900/2850]\n",
      "loss: 3.485214  [1000/2850]\n",
      "loss: 3.667395  [1100/2850]\n",
      "loss: 3.938506  [1200/2850]\n",
      "loss: 3.619253  [1300/2850]\n",
      "loss: 3.462740  [1400/2850]\n",
      "loss: 3.579226  [1500/2850]\n",
      "loss: 3.660937  [1600/2850]\n",
      "loss: 3.674235  [1700/2850]\n",
      "loss: 3.239806  [1800/2850]\n",
      "loss: 3.553093  [1900/2850]\n",
      "loss: 3.730490  [2000/2850]\n",
      "loss: 3.831514  [2100/2850]\n",
      "loss: 3.391171  [2200/2850]\n",
      "loss: 3.764959  [2300/2850]\n",
      "loss: 3.279122  [2400/2850]\n",
      "loss: 3.550979  [2500/2850]\n",
      "loss: 3.760540  [2600/2850]\n",
      "loss: 3.669767  [2700/2850]\n",
      "loss: 3.360156  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.2%, Avg loss: 3.528621 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.532460  [  0/2850]\n",
      "loss: 3.566267  [100/2850]\n",
      "loss: 3.704149  [200/2850]\n",
      "loss: 3.435701  [300/2850]\n",
      "loss: 3.117241  [400/2850]\n",
      "loss: 3.095437  [500/2850]\n",
      "loss: 3.695375  [600/2850]\n",
      "loss: 3.364511  [700/2850]\n",
      "loss: 3.865146  [800/2850]\n",
      "loss: 3.337543  [900/2850]\n",
      "loss: 3.146300  [1000/2850]\n",
      "loss: 3.090255  [1100/2850]\n",
      "loss: 3.867179  [1200/2850]\n",
      "loss: 3.475368  [1300/2850]\n",
      "loss: 3.317627  [1400/2850]\n",
      "loss: 3.256523  [1500/2850]\n",
      "loss: 3.503919  [1600/2850]\n",
      "loss: 3.686935  [1700/2850]\n",
      "loss: 3.332277  [1800/2850]\n",
      "loss: 3.572623  [1900/2850]\n",
      "loss: 3.084452  [2000/2850]\n",
      "loss: 3.590110  [2100/2850]\n",
      "loss: 3.772071  [2200/2850]\n",
      "loss: 3.326083  [2300/2850]\n",
      "loss: 3.787362  [2400/2850]\n",
      "loss: 3.155174  [2500/2850]\n",
      "loss: 3.491706  [2600/2850]\n",
      "loss: 3.573648  [2700/2850]\n",
      "loss: 3.814078  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.9%, Avg loss: 3.528864 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.440660  [  0/2850]\n",
      "loss: 3.300261  [100/2850]\n",
      "loss: 3.442530  [200/2850]\n",
      "loss: 3.475226  [300/2850]\n",
      "loss: 3.986385  [400/2850]\n",
      "loss: 3.345690  [500/2850]\n",
      "loss: 3.344555  [600/2850]\n",
      "loss: 3.307391  [700/2850]\n",
      "loss: 3.112554  [800/2850]\n",
      "loss: 3.110166  [900/2850]\n",
      "loss: 3.388154  [1000/2850]\n",
      "loss: 3.391511  [1100/2850]\n",
      "loss: 3.630626  [1200/2850]\n",
      "loss: 3.598256  [1300/2850]\n",
      "loss: 3.693165  [1400/2850]\n",
      "loss: 3.186296  [1500/2850]\n",
      "loss: 3.703768  [1600/2850]\n",
      "loss: 3.684115  [1700/2850]\n",
      "loss: 3.501302  [1800/2850]\n",
      "loss: 3.479861  [1900/2850]\n",
      "loss: 3.440892  [2000/2850]\n",
      "loss: 3.125230  [2100/2850]\n",
      "loss: 3.616548  [2200/2850]\n",
      "loss: 3.450120  [2300/2850]\n",
      "loss: 4.109152  [2400/2850]\n",
      "loss: 3.873944  [2500/2850]\n",
      "loss: 3.839523  [2600/2850]\n",
      "loss: 3.551502  [2700/2850]\n",
      "loss: 3.728053  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 16.2%, Avg loss: 3.502972 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.297472  [  0/2850]\n",
      "loss: 3.339599  [100/2850]\n",
      "loss: 3.842137  [200/2850]\n",
      "loss: 3.491496  [300/2850]\n",
      "loss: 3.885197  [400/2850]\n",
      "loss: 3.699009  [500/2850]\n",
      "loss: 3.506861  [600/2850]\n",
      "loss: 3.365483  [700/2850]\n",
      "loss: 3.773969  [800/2850]\n",
      "loss: 3.121369  [900/2850]\n",
      "loss: 3.136752  [1000/2850]\n",
      "loss: 3.771995  [1100/2850]\n",
      "loss: 3.248472  [1200/2850]\n",
      "loss: 3.360804  [1300/2850]\n",
      "loss: 4.135943  [1400/2850]\n",
      "loss: 3.636697  [1500/2850]\n",
      "loss: 3.617269  [1600/2850]\n",
      "loss: 3.297991  [1700/2850]\n",
      "loss: 3.393851  [1800/2850]\n",
      "loss: 3.393431  [1900/2850]\n",
      "loss: 3.624088  [2000/2850]\n",
      "loss: 3.636560  [2100/2850]\n",
      "loss: 3.682693  [2200/2850]\n",
      "loss: 3.422067  [2300/2850]\n",
      "loss: 3.377760  [2400/2850]\n",
      "loss: 3.119587  [2500/2850]\n",
      "loss: 3.234311  [2600/2850]\n",
      "loss: 3.960540  [2700/2850]\n",
      "loss: 3.092791  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.4%, Avg loss: 3.542177 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.465833  [  0/2850]\n",
      "loss: 3.190923  [100/2850]\n",
      "loss: 3.529793  [200/2850]\n",
      "loss: 3.563679  [300/2850]\n",
      "loss: 3.996752  [400/2850]\n",
      "loss: 2.976099  [500/2850]\n",
      "loss: 3.990364  [600/2850]\n",
      "loss: 3.400930  [700/2850]\n",
      "loss: 3.388580  [800/2850]\n",
      "loss: 3.439688  [900/2850]\n",
      "loss: 3.236174  [1000/2850]\n",
      "loss: 3.842364  [1100/2850]\n",
      "loss: 4.055072  [1200/2850]\n",
      "loss: 3.246286  [1300/2850]\n",
      "loss: 3.250235  [1400/2850]\n",
      "loss: 3.512801  [1500/2850]\n",
      "loss: 3.332211  [1600/2850]\n",
      "loss: 3.577161  [1700/2850]\n",
      "loss: 3.659694  [1800/2850]\n",
      "loss: 3.676531  [1900/2850]\n",
      "loss: 3.389050  [2000/2850]\n",
      "loss: 3.519248  [2100/2850]\n",
      "loss: 3.770283  [2200/2850]\n",
      "loss: 3.151292  [2300/2850]\n",
      "loss: 3.800366  [2400/2850]\n",
      "loss: 4.043538  [2500/2850]\n",
      "loss: 3.337363  [2600/2850]\n",
      "loss: 3.313891  [2700/2850]\n",
      "loss: 3.505439  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.9%, Avg loss: 3.503095 \n",
      "\n",
      "Done!\n",
      "Predicted class probabilities: [[ -3.0935705   -1.8288233   -5.719177     0.28221908  -6.830163\n",
      "   -1.8728955  -15.021413    -8.746348    -2.521329     0.99563533\n",
      "   -6.1232963   -5.005893    -6.7876477   -5.843218    -3.5386806\n",
      "    1.2556477   -8.586572    -2.8001268    1.1981909   -4.6450577\n",
      "    1.0806673    2.2579703  -10.310246    -4.902285    -5.968135\n",
      "   -7.648967    -3.0096464   -6.5887175   -2.302196    -5.202386\n",
      "   -3.9337401   -0.92420375  -2.5161564    1.9968419   -3.4620948\n",
      "   -8.53102      1.2646388    3.4919624   -3.975126    -1.8775463\n",
      "  -10.8390045   -1.5008919   -9.980022    -3.4392302  -10.338688\n",
      "   -3.1778495   -8.00276     -6.295206     1.4294361   -9.426006\n",
      "  -18.48688      1.5479777   -9.8216715   -3.474029    -7.816687\n",
      "   -3.0983763   -1.0724806   -1.009681    -3.8154361  -10.454391\n",
      "    3.5195706  -11.811023    -5.245049    -7.1321297   -5.6723595\n",
      "   -1.490303    -1.7400581   -0.2733955    0.27143773  -2.8326204\n",
      "   -6.04692     -5.7348404   -7.8441586   -4.8346386   -3.1830707\n",
      "   -9.441081   -13.925656    -4.040531    -7.6512403  -10.126402\n",
      "   -0.4127407   -3.4278767  -10.347612    -5.3581967   -8.876628\n",
      "   -5.8000727   -6.5099254   -0.3791339    0.42703152   0.7233672\n",
      "   -3.3499613   -8.071859    -7.5192494   -2.2773418    2.4772413\n",
      "   -6.547564    -4.7624345   -8.875564    -1.7247877   -1.8542306\n",
      "   -4.9926453  -13.760698    -1.2212598   -0.3990757   -0.93772024\n",
      "   -0.6135825   -0.44792882  -3.0332446    0.59788895  -7.607936\n",
      "   -5.7330375   -2.5506349    0.8325681   -7.9268417 ]]\n"
     ]
    }
   ],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.dataset as ds\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor, Model\n",
    "import mindspore.dataset.vision as vision\n",
    "import mindspore.dataset.transforms as transforms\n",
    "from mindspore.train.callback import LossMonitor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: 加载并处理数据集\n",
    "df = pd.read_csv(\"processed_dataset.csv\")\n",
    "\n",
    "# 将 'explicit' 列转换为数值型 (True/False 转换为 1/0)\n",
    "df['explicit'] = df['explicit'].astype(int)\n",
    "\n",
    "# 将类别特征 'track_genre' 做 One-Hot 编码\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "track_genre_encoded = encoder.fit_transform(df[['track_genre']])\n",
    "\n",
    "# 将编码后的数据添加回数据框并移除原始的 'track_genre' 列\n",
    "encoded_columns = encoder.get_feature_names_out(['track_genre'])\n",
    "df_encoded = pd.DataFrame(track_genre_encoded, columns=encoded_columns)\n",
    "df = pd.concat([df.drop(columns=['track_genre']), df_encoded], axis=1)\n",
    "\n",
    "# 对类别特征 'key', 'mode', 'time_signature' 进行 One-Hot 编码\n",
    "categorical_features = ['key', 'mode', 'time_signature']\n",
    "categorical_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "categorical_encoded = categorical_encoder.fit_transform(df[categorical_features])\n",
    "categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=categorical_encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# 合并编码后的数据和数值特征，并移除原始的类别特征\n",
    "df = pd.concat([df.drop(columns=categorical_features).reset_index(drop=True), categorical_encoded_df], axis=1)\n",
    "\n",
    "# 标准化数值特征\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64'])\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features.columns] = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# 将标签转换为 float32 类型\n",
    "df_encoded = df_encoded.astype(np.float32)\n",
    "\n",
    "# 提取特征和标签\n",
    "X = df.drop(columns=encoded_columns).astype(np.float32).values\n",
    "y = df_encoded.values\n",
    "\n",
    "# Step 2: 定义一个简单的 MLP 分类器\n",
    "class MLPClassifier(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # 输入层到第一层隐藏层的全连接层\n",
    "        self.fc1 = nn.Dense(input_size, hidden_sizes[0], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # 第一层隐藏层到第二层隐藏层的全连接层\n",
    "        self.fc2 = nn.Dense(hidden_sizes[0], hidden_sizes[1], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # 第二层隐藏层到输出层的全连接层\n",
    "        self.fc3 = nn.Dense(hidden_sizes[1], output_size, weight_init=\"normal\")\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 数据通过输入层进入第一层隐藏层\n",
    "        x = self.fc1(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu1(x)\n",
    "        # 数据通过第一层隐藏层进入第二层隐藏层\n",
    "        x = self.fc2(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu2(x)\n",
    "        # 数据通过第二层隐藏层进入输出层\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: 创建模型实例\n",
    "input_size = X.shape[1]  # 输入层的特征数量\n",
    "hidden_sizes = [100, 50]  # 隐藏层中的神经元数量\n",
    "output_size = y.shape[1]  # 输出层的神经元数量（类别数量）\n",
    "model = MLPClassifier(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Step 4: 定义超参数、损失函数和优化器\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction='mean')  # 损失函数\n",
    "optimizer = nn.Adam(params=model.trainable_params(), learning_rate=learning_rate)  # 优化器\n",
    "\n",
    "# Step 5: 割分训练集和测试集\n",
    "# 将数据集割分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_dataset = ds.NumpySlicesDataset(data=(X_train, y_train), column_names=['features', 'labels'], shuffle=True).batch(batch_size)\n",
    "test_dataset = ds.NumpySlicesDataset(data=(X_test, y_test), column_names=['features', 'labels'], shuffle=False).batch(batch_size)\n",
    "\n",
    "# Step 6: 定义训练和测试函数\n",
    "def forward_fn(data, label):\n",
    "    logits = model(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss, logits\n",
    "\n",
    "grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "def train_step(data, label):\n",
    "    (loss, _), grads = grad_fn(data, label)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def train_loop(model, dataset, loss_fn):\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "        if batch % 100 == 0:\n",
    "            loss_value = loss.asnumpy()\n",
    "            print(f\"loss: {loss_value:>7f}  [{batch:>3d}/{size:>3d}]\")\n",
    "\n",
    "def test_loop(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label.argmax(1)).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Step 7: 开始训练与测试\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(model, train_dataset, loss_fn)\n",
    "    test_loop(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Step 8: 使用模型进行推理\n",
    "# 创建一个形状为 (1, input_size) 的全为 1 的形状作为输入\n",
    "X_infer = Tensor(np.ones((1, input_size)), ms.float32)\n",
    "# 使用模型进行前向传播，获取输出 logits\n",
    "y_pred = model(X_infer)\n",
    "# 打印预测结果\n",
    "print(f\"Predicted class probabilities: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4e087f-940d-4939-9fc2-bfeec436aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:37:41.497.224 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2310715098.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:37:41.497.269 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2310715098.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:37:41.497.288 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2310715098.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:37:41.497.308 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2310715098.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.751053  [  0/2850]\n",
      "loss: 4.381146  [100/2850]\n",
      "loss: 3.770947  [200/2850]\n",
      "loss: 3.517884  [300/2850]\n",
      "loss: 4.052758  [400/2850]\n",
      "loss: 3.977578  [500/2850]\n",
      "loss: 3.172975  [600/2850]\n",
      "loss: 3.694036  [700/2850]\n",
      "loss: 4.385149  [800/2850]\n",
      "loss: 3.965744  [900/2850]\n",
      "loss: 3.912257  [1000/2850]\n",
      "loss: 3.987021  [1100/2850]\n",
      "loss: 4.232032  [1200/2850]\n",
      "loss: 3.177134  [1300/2850]\n",
      "loss: 3.458052  [1400/2850]\n",
      "loss: 4.127619  [1500/2850]\n",
      "loss: 3.224372  [1600/2850]\n",
      "loss: 3.719158  [1700/2850]\n",
      "loss: 3.849706  [1800/2850]\n",
      "loss: 3.815284  [1900/2850]\n",
      "loss: 3.405531  [2000/2850]\n",
      "loss: 3.632096  [2100/2850]\n",
      "loss: 3.390835  [2200/2850]\n",
      "loss: 3.478814  [2300/2850]\n",
      "loss: 3.842928  [2400/2850]\n",
      "loss: 3.734197  [2500/2850]\n",
      "loss: 3.250527  [2600/2850]\n",
      "loss: 3.887791  [2700/2850]\n",
      "loss: 3.505733  [2800/2850]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:37:49.653.123 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2310715098.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:37:49.653.197 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2310715098.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:37:51.649.432 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2310715098.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:37:51.649.471 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2310715098.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      " Accuracy: 14.5%, Avg loss: 3.557605 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 4.062263  [  0/2850]\n",
      "loss: 3.758794  [100/2850]\n",
      "loss: 3.526545  [200/2850]\n",
      "loss: 3.984953  [300/2850]\n",
      "loss: 3.641108  [400/2850]\n",
      "loss: 4.017857  [500/2850]\n",
      "loss: 3.696941  [600/2850]\n",
      "loss: 3.560792  [700/2850]\n",
      "loss: 3.940561  [800/2850]\n",
      "loss: 3.793410  [900/2850]\n",
      "loss: 3.546626  [1000/2850]\n",
      "loss: 3.499299  [1100/2850]\n",
      "loss: 3.584493  [1200/2850]\n",
      "loss: 3.412812  [1300/2850]\n",
      "loss: 3.321845  [1400/2850]\n",
      "loss: 3.223374  [1500/2850]\n",
      "loss: 3.533934  [1600/2850]\n",
      "loss: 3.495475  [1700/2850]\n",
      "loss: 3.511067  [1800/2850]\n",
      "loss: 3.160665  [1900/2850]\n",
      "loss: 3.463381  [2000/2850]\n",
      "loss: 3.674786  [2100/2850]\n",
      "loss: 3.719467  [2200/2850]\n",
      "loss: 3.383220  [2300/2850]\n",
      "loss: 3.703263  [2400/2850]\n",
      "loss: 3.462914  [2500/2850]\n",
      "loss: 3.441289  [2600/2850]\n",
      "loss: 3.415045  [2700/2850]\n",
      "loss: 3.958128  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 14.7%, Avg loss: 3.551239 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.895353  [  0/2850]\n",
      "loss: 3.731111  [100/2850]\n",
      "loss: 4.135305  [200/2850]\n",
      "loss: 3.406402  [300/2850]\n",
      "loss: 3.119632  [400/2850]\n",
      "loss: 3.753871  [500/2850]\n",
      "loss: 3.865802  [600/2850]\n",
      "loss: 2.988653  [700/2850]\n",
      "loss: 3.599559  [800/2850]\n",
      "loss: 3.022467  [900/2850]\n",
      "loss: 4.152885  [1000/2850]\n",
      "loss: 3.463048  [1100/2850]\n",
      "loss: 3.678980  [1200/2850]\n",
      "loss: 3.696511  [1300/2850]\n",
      "loss: 3.118029  [1400/2850]\n",
      "loss: 3.315912  [1500/2850]\n",
      "loss: 3.369213  [1600/2850]\n",
      "loss: 4.028014  [1700/2850]\n",
      "loss: 3.091978  [1800/2850]\n",
      "loss: 3.975439  [1900/2850]\n",
      "loss: 3.853755  [2000/2850]\n",
      "loss: 3.657447  [2100/2850]\n",
      "loss: 3.344846  [2200/2850]\n",
      "loss: 3.239000  [2300/2850]\n",
      "loss: 3.976202  [2400/2850]\n",
      "loss: 3.677224  [2500/2850]\n",
      "loss: 3.119690  [2600/2850]\n",
      "loss: 3.454845  [2700/2850]\n",
      "loss: 3.532572  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.5%, Avg loss: 3.523772 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.441571  [  0/2850]\n",
      "loss: 3.512527  [100/2850]\n",
      "loss: 3.241196  [200/2850]\n",
      "loss: 3.975446  [300/2850]\n",
      "loss: 4.185207  [400/2850]\n",
      "loss: 3.375050  [500/2850]\n",
      "loss: 3.509598  [600/2850]\n",
      "loss: 3.539635  [700/2850]\n",
      "loss: 3.607907  [800/2850]\n",
      "loss: 3.171362  [900/2850]\n",
      "loss: 3.517320  [1000/2850]\n",
      "loss: 3.234233  [1100/2850]\n",
      "loss: 3.161587  [1200/2850]\n",
      "loss: 3.559044  [1300/2850]\n",
      "loss: 3.467643  [1400/2850]\n",
      "loss: 3.700939  [1500/2850]\n",
      "loss: 3.369615  [1600/2850]\n",
      "loss: 3.591320  [1700/2850]\n",
      "loss: 3.314452  [1800/2850]\n",
      "loss: 3.378524  [1900/2850]\n",
      "loss: 3.413508  [2000/2850]\n",
      "loss: 3.362641  [2100/2850]\n",
      "loss: 3.992819  [2200/2850]\n",
      "loss: 3.748266  [2300/2850]\n",
      "loss: 3.317546  [2400/2850]\n",
      "loss: 3.523796  [2500/2850]\n",
      "loss: 3.496505  [2600/2850]\n",
      "loss: 3.579397  [2700/2850]\n",
      "loss: 4.040438  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.7%, Avg loss: 3.516987 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.416212  [  0/2850]\n",
      "loss: 3.619080  [100/2850]\n",
      "loss: 3.333057  [200/2850]\n",
      "loss: 3.265527  [300/2850]\n",
      "loss: 2.985384  [400/2850]\n",
      "loss: 3.485829  [500/2850]\n",
      "loss: 3.664235  [600/2850]\n",
      "loss: 3.708397  [700/2850]\n",
      "loss: 3.822247  [800/2850]\n",
      "loss: 3.278845  [900/2850]\n",
      "loss: 3.440827  [1000/2850]\n",
      "loss: 3.352225  [1100/2850]\n",
      "loss: 3.121883  [1200/2850]\n",
      "loss: 3.317929  [1300/2850]\n",
      "loss: 3.694924  [1400/2850]\n",
      "loss: 3.777520  [1500/2850]\n",
      "loss: 3.009034  [1600/2850]\n",
      "loss: 3.519192  [1700/2850]\n",
      "loss: 3.654508  [1800/2850]\n",
      "loss: 3.575815  [1900/2850]\n",
      "loss: 3.745895  [2000/2850]\n",
      "loss: 2.923446  [2100/2850]\n",
      "loss: 3.427424  [2200/2850]\n",
      "loss: 3.361874  [2300/2850]\n",
      "loss: 3.232935  [2400/2850]\n",
      "loss: 3.146920  [2500/2850]\n",
      "loss: 3.463222  [2600/2850]\n",
      "loss: 3.762118  [2700/2850]\n",
      "loss: 2.900365  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 16.2%, Avg loss: 3.495763 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.506244  [  0/2850]\n",
      "loss: 3.880263  [100/2850]\n",
      "loss: 3.795337  [200/2850]\n",
      "loss: 3.416962  [300/2850]\n",
      "loss: 3.624084  [400/2850]\n",
      "loss: 3.168958  [500/2850]\n",
      "loss: 3.739605  [600/2850]\n",
      "loss: 3.329978  [700/2850]\n",
      "loss: 3.462609  [800/2850]\n",
      "loss: 3.552156  [900/2850]\n",
      "loss: 3.355896  [1000/2850]\n",
      "loss: 3.458458  [1100/2850]\n",
      "loss: 3.418102  [1200/2850]\n",
      "loss: 3.215360  [1300/2850]\n",
      "loss: 3.695278  [1400/2850]\n",
      "loss: 4.018632  [1500/2850]\n",
      "loss: 3.295865  [1600/2850]\n",
      "loss: 3.340659  [1700/2850]\n",
      "loss: 3.882712  [1800/2850]\n",
      "loss: 3.694940  [1900/2850]\n",
      "loss: 3.429288  [2000/2850]\n",
      "loss: 3.253174  [2100/2850]\n",
      "loss: 3.677742  [2200/2850]\n",
      "loss: 3.267026  [2300/2850]\n",
      "loss: 3.590317  [2400/2850]\n",
      "loss: 3.650834  [2500/2850]\n",
      "loss: 3.633399  [2600/2850]\n",
      "loss: 3.501997  [2700/2850]\n",
      "loss: 3.150287  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.7%, Avg loss: 3.519956 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.893005  [  0/2850]\n",
      "loss: 3.507346  [100/2850]\n",
      "loss: 3.078824  [200/2850]\n",
      "loss: 3.128977  [300/2850]\n",
      "loss: 3.547855  [400/2850]\n",
      "loss: 2.856127  [500/2850]\n",
      "loss: 3.766744  [600/2850]\n",
      "loss: 3.585897  [700/2850]\n",
      "loss: 3.489566  [800/2850]\n",
      "loss: 3.287204  [900/2850]\n",
      "loss: 3.748734  [1000/2850]\n",
      "loss: 4.068045  [1100/2850]\n",
      "loss: 3.337973  [1200/2850]\n",
      "loss: 3.563059  [1300/2850]\n",
      "loss: 3.563742  [1400/2850]\n",
      "loss: 3.873009  [1500/2850]\n",
      "loss: 3.106563  [1600/2850]\n",
      "loss: 3.682965  [1700/2850]\n",
      "loss: 3.648701  [1800/2850]\n",
      "loss: 2.946033  [1900/2850]\n",
      "loss: 3.445409  [2000/2850]\n",
      "loss: 3.734891  [2100/2850]\n",
      "loss: 3.790143  [2200/2850]\n",
      "loss: 3.507469  [2300/2850]\n",
      "loss: 3.708731  [2400/2850]\n",
      "loss: 3.539517  [2500/2850]\n",
      "loss: 3.295943  [2600/2850]\n",
      "loss: 3.794262  [2700/2850]\n",
      "loss: 3.676995  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.7%, Avg loss: 3.514672 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.436850  [  0/2850]\n",
      "loss: 3.328813  [100/2850]\n",
      "loss: 3.717127  [200/2850]\n",
      "loss: 3.712561  [300/2850]\n",
      "loss: 3.428048  [400/2850]\n",
      "loss: 3.327885  [500/2850]\n",
      "loss: 3.549917  [600/2850]\n",
      "loss: 3.720772  [700/2850]\n",
      "loss: 3.429845  [800/2850]\n",
      "loss: 3.387461  [900/2850]\n",
      "loss: 3.085816  [1000/2850]\n",
      "loss: 3.471760  [1100/2850]\n",
      "loss: 3.425866  [1200/2850]\n",
      "loss: 3.532501  [1300/2850]\n",
      "loss: 3.535533  [1400/2850]\n",
      "loss: 4.159371  [1500/2850]\n",
      "loss: 3.161353  [1600/2850]\n",
      "loss: 3.611940  [1700/2850]\n",
      "loss: 3.903015  [1800/2850]\n",
      "loss: 3.477466  [1900/2850]\n",
      "loss: 3.885144  [2000/2850]\n",
      "loss: 3.825199  [2100/2850]\n",
      "loss: 3.724240  [2200/2850]\n",
      "loss: 3.445558  [2300/2850]\n",
      "loss: 3.459761  [2400/2850]\n",
      "loss: 3.211891  [2500/2850]\n",
      "loss: 3.435522  [2600/2850]\n",
      "loss: 3.429029  [2700/2850]\n",
      "loss: 3.585200  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.7%, Avg loss: 3.521290 \n",
      "\n",
      "Done!\n",
      "Predicted class probabilities: [[-10.782884  -16.139105   -9.920397   -6.504541  -35.450188   -7.799408\n",
      "  -17.470686  -15.749432  -13.687595   -7.8408027 -15.464938   -9.642245\n",
      "  -27.927822  -24.108204  -26.549446  -12.339778  -59.437492   -8.271553\n",
      "   -4.3680124  -8.098137   -7.139329   -6.6248713 -74.209496  -11.7379055\n",
      "  -34.73349   -18.258776  -18.528296  -12.403871   -8.646689   -8.343485\n",
      "   -6.4573593  -5.3119516  -7.663759   -6.7725844 -18.531965   -2.0075612\n",
      "   -6.875983   -2.2834885  -7.3682075  -3.8586454 -27.071907  -12.862457\n",
      "  -83.80294    -7.989632  -12.783889  -21.367777   -5.924097  -13.458809\n",
      "  -10.201132   -5.4144363 -16.155529   -4.037625  -46.23748    -4.858449\n",
      "  -15.941237  -13.368344   -9.532651   -8.948716  -12.762646  -13.399518\n",
      "   -6.101779  -14.4230795  -9.691438   -9.709435  -21.961056   -7.6084137\n",
      "  -17.826439   -9.7008095  -7.3132596 -16.033997  -18.817839  -11.106668\n",
      "  -15.576876  -40.070396  -23.18145   -33.239323  -28.047272   -9.81611\n",
      "   -1.7603335 -12.444504  -10.805669  -24.719608  -10.382675   -6.122925\n",
      "  -16.411259   -4.3138814  -5.275325   -1.6656264 -12.271701   -9.749055\n",
      "   -8.210169  -14.337005  -12.811608  -27.24188   -10.444792  -20.596031\n",
      "  -21.26382    -9.387157  -12.295485  -11.291827  -10.251433  -89.14472\n",
      "  -13.470682   -5.4999504 -10.741294  -45.778103   -6.6763477 -11.790821\n",
      "  -31.950253   -9.82139   -18.847776  -16.898882   -7.534356  -30.539383 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:39:04.771.351 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2310715098.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:39:04.771.393 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2310715098.py]\n"
     ]
    }
   ],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.dataset as ds\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor, Model,context\n",
    "import mindspore.dataset.vision as vision\n",
    "import mindspore.dataset.transforms as transforms\n",
    "from mindspore.train.callback import LossMonitor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 设置运行环境为 GPU\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")\n",
    "# Step 1: 加载并处理数据集\n",
    "df = pd.read_csv(\"processed_dataset.csv\")\n",
    "\n",
    "# 将 'explicit' 列转换为数值型 (True/False 转换为 1/0)\n",
    "df['explicit'] = df['explicit'].astype(int)\n",
    "\n",
    "# 将类别特征 'track_genre' 做 One-Hot 编码\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "track_genre_encoded = encoder.fit_transform(df[['track_genre']])\n",
    "\n",
    "# 将编码后的数据添加回数据框并移除原始的 'track_genre' 列\n",
    "encoded_columns = encoder.get_feature_names_out(['track_genre'])\n",
    "df_encoded = pd.DataFrame(track_genre_encoded, columns=encoded_columns)\n",
    "df = pd.concat([df.drop(columns=['track_genre']), df_encoded], axis=1)\n",
    "\n",
    "# 对类别特征 'key', 'mode', 'time_signature' 进行 One-Hot 编码\n",
    "categorical_features = ['key', 'mode', 'time_signature']\n",
    "categorical_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "categorical_encoded = categorical_encoder.fit_transform(df[categorical_features])\n",
    "categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=categorical_encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# 合并编码后的数据和数值特征，并移除原始的类别特征\n",
    "df = pd.concat([df.drop(columns=categorical_features).reset_index(drop=True), categorical_encoded_df], axis=1)\n",
    "\n",
    "# 标准化数值特征\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64'])\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features.columns] = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# 将标签转换为 float32 类型\n",
    "df_encoded = df_encoded.astype(np.float32)\n",
    "\n",
    "# 提取特征和标签\n",
    "X = df.drop(columns=encoded_columns).astype(np.float32).values\n",
    "y = df_encoded.values\n",
    "\n",
    "# Step 2: 定义一个简单的 MLP 分类器\n",
    "class MLPClassifier(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # 输入层到第一层隐藏层的全连接层\n",
    "        self.fc1 = nn.Dense(input_size, hidden_sizes[0], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # 第一层隐藏层到第二层隐藏层的全连接层\n",
    "        self.fc2 = nn.Dense(hidden_sizes[0], hidden_sizes[1], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # 第二层隐藏层到输出层的全连接层\n",
    "        self.fc3 = nn.Dense(hidden_sizes[1], output_size, weight_init=\"normal\")\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 数据通过输入层进入第一层隐藏层\n",
    "        x = self.fc1(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu1(x)\n",
    "        # 数据通过第一层隐藏层进入第二层隐藏层\n",
    "        x = self.fc2(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu2(x)\n",
    "        # 数据通过第二层隐藏层进入输出层\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: 创建模型实例\n",
    "input_size = X.shape[1]  # 输入层的特征数量\n",
    "hidden_sizes = [50, 100]  # 隐藏层中的神经元数量\n",
    "output_size = y.shape[1]  # 输出层的神经元数量（类别数量）\n",
    "model = MLPClassifier(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Step 4: 定义超参数、损失函数和优化器\n",
    "epochs = 8\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction='mean')  # 损失函数\n",
    "optimizer = nn.Adam(params=model.trainable_params(), learning_rate=learning_rate)  # 优化器\n",
    "\n",
    "# Step 5: 割分训练集和测试集\n",
    "# 将数据集割分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_dataset = ds.NumpySlicesDataset(data=(X_train, y_train), column_names=['features', 'labels'], shuffle=True).batch(batch_size)\n",
    "test_dataset = ds.NumpySlicesDataset(data=(X_test, y_test), column_names=['features', 'labels'], shuffle=False).batch(batch_size)\n",
    "\n",
    "# Step 6: 定义训练和测试函数\n",
    "def forward_fn(data, label):\n",
    "    logits = model(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss, logits\n",
    "\n",
    "grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "def train_step(data, label):\n",
    "    (loss, _), grads = grad_fn(data, label)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def train_loop(model, dataset, loss_fn):\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "        if batch % 100 == 0:\n",
    "            loss_value = loss.asnumpy()\n",
    "            print(f\"loss: {loss_value:>7f}  [{batch:>3d}/{size:>3d}]\")\n",
    "\n",
    "def test_loop(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label.argmax(1)).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Step 7: 开始训练与测试\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(model, train_dataset, loss_fn)\n",
    "    test_loop(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Step 8: 使用模型进行推理\n",
    "# 创建一个形状为 (1, input_size) 的全为 1 的形状作为输入\n",
    "X_infer = Tensor(np.ones((1, input_size)), ms.float32)\n",
    "# 使用模型进行前向传播，获取输出 logits\n",
    "y_pred = model(X_infer)\n",
    "# 打印预测结果\n",
    "print(f\"Predicted class probabilities: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99065b58-9cbd-4e76-b464-6e254da73ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:40:15.432.774 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2521907368.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:40:15.432.811 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2521907368.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:40:15.432.828 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2521907368.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:40:15.432.857 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2521907368.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.739250  [  0/1425]\n",
      "loss: 3.901616  [100/1425]\n",
      "loss: 3.844621  [200/1425]\n",
      "loss: 3.843393  [300/1425]\n",
      "loss: 3.574919  [400/1425]\n",
      "loss: 3.427258  [500/1425]\n",
      "loss: 3.609534  [600/1425]\n",
      "loss: 3.589840  [700/1425]\n",
      "loss: 3.652241  [800/1425]\n",
      "loss: 3.453996  [900/1425]\n",
      "loss: 3.672565  [1000/1425]\n",
      "loss: 3.564564  [1100/1425]\n",
      "loss: 3.459152  [1200/1425]\n",
      "loss: 3.507923  [1300/1425]\n",
      "loss: 3.547889  [1400/1425]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:40:22.991.193 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2521907368.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:40:22.991.238 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2521907368.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:40:25.208.620 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2521907368.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:40:25.208.663 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2521907368.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      " Accuracy: 15.3%, Avg loss: 3.515040 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.212141  [  0/1425]\n",
      "loss: 3.419340  [100/1425]\n",
      "loss: 3.615645  [200/1425]\n",
      "loss: 3.557674  [300/1425]\n",
      "loss: 3.624898  [400/1425]\n",
      "loss: 3.495601  [500/1425]\n",
      "loss: 3.933043  [600/1425]\n",
      "loss: 3.223706  [700/1425]\n",
      "loss: 3.419834  [800/1425]\n",
      "loss: 3.515543  [900/1425]\n",
      "loss: 3.309186  [1000/1425]\n",
      "loss: 3.384953  [1100/1425]\n",
      "loss: 3.563389  [1200/1425]\n",
      "loss: 3.347105  [1300/1425]\n",
      "loss: 3.410980  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 16.8%, Avg loss: 3.448343 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.180415  [  0/1425]\n",
      "loss: 3.396326  [100/1425]\n",
      "loss: 3.157975  [200/1425]\n",
      "loss: 3.159078  [300/1425]\n",
      "loss: 3.218615  [400/1425]\n",
      "loss: 3.291227  [500/1425]\n",
      "loss: 3.338558  [600/1425]\n",
      "loss: 3.184371  [700/1425]\n",
      "loss: 3.424456  [800/1425]\n",
      "loss: 3.255927  [900/1425]\n",
      "loss: 3.534780  [1000/1425]\n",
      "loss: 3.312005  [1100/1425]\n",
      "loss: 3.347055  [1200/1425]\n",
      "loss: 3.405071  [1300/1425]\n",
      "loss: 3.374568  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 16.5%, Avg loss: 3.463197 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.349942  [  0/1425]\n",
      "loss: 3.835930  [100/1425]\n",
      "loss: 3.163936  [200/1425]\n",
      "loss: 3.765988  [300/1425]\n",
      "loss: 3.570749  [400/1425]\n",
      "loss: 3.436855  [500/1425]\n",
      "loss: 3.092833  [600/1425]\n",
      "loss: 3.780664  [700/1425]\n",
      "loss: 3.381289  [800/1425]\n",
      "loss: 3.233556  [900/1425]\n",
      "loss: 3.397744  [1000/1425]\n",
      "loss: 3.494360  [1100/1425]\n",
      "loss: 3.572060  [1200/1425]\n",
      "loss: 3.218694  [1300/1425]\n",
      "loss: 3.722900  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 17.2%, Avg loss: 3.427402 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.437602  [  0/1425]\n",
      "loss: 3.105823  [100/1425]\n",
      "loss: 3.616837  [200/1425]\n",
      "loss: 3.300806  [300/1425]\n",
      "loss: 3.589012  [400/1425]\n",
      "loss: 3.141899  [500/1425]\n",
      "loss: 3.585733  [600/1425]\n",
      "loss: 3.319708  [700/1425]\n",
      "loss: 3.223621  [800/1425]\n",
      "loss: 3.384359  [900/1425]\n",
      "loss: 3.318564  [1000/1425]\n",
      "loss: 3.741130  [1100/1425]\n",
      "loss: 2.986471  [1200/1425]\n",
      "loss: 3.005856  [1300/1425]\n",
      "loss: 3.418557  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 17.2%, Avg loss: 3.424329 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.390043  [  0/1425]\n",
      "loss: 3.173253  [100/1425]\n",
      "loss: 3.388420  [200/1425]\n",
      "loss: 3.474468  [300/1425]\n",
      "loss: 3.408944  [400/1425]\n",
      "loss: 3.619491  [500/1425]\n",
      "loss: 3.432953  [600/1425]\n",
      "loss: 3.319082  [700/1425]\n",
      "loss: 3.248275  [800/1425]\n",
      "loss: 3.376111  [900/1425]\n",
      "loss: 3.566627  [1000/1425]\n",
      "loss: 3.186209  [1100/1425]\n",
      "loss: 3.196520  [1200/1425]\n",
      "loss: 3.482195  [1300/1425]\n",
      "loss: 4.009511  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 17.1%, Avg loss: 3.423603 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.522229  [  0/1425]\n",
      "loss: 3.578218  [100/1425]\n",
      "loss: 3.210195  [200/1425]\n",
      "loss: 3.383860  [300/1425]\n",
      "loss: 3.481287  [400/1425]\n",
      "loss: 3.095080  [500/1425]\n",
      "loss: 3.479940  [600/1425]\n",
      "loss: 3.128189  [700/1425]\n",
      "loss: 3.255516  [800/1425]\n",
      "loss: 3.304408  [900/1425]\n",
      "loss: 3.353727  [1000/1425]\n",
      "loss: 3.571329  [1100/1425]\n",
      "loss: 3.487277  [1200/1425]\n",
      "loss: 3.469561  [1300/1425]\n",
      "loss: 3.586059  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 17.2%, Avg loss: 3.404587 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.492014  [  0/1425]\n",
      "loss: 3.188497  [100/1425]\n",
      "loss: 3.416701  [200/1425]\n",
      "loss: 3.989293  [300/1425]\n",
      "loss: 3.336938  [400/1425]\n",
      "loss: 3.229751  [500/1425]\n",
      "loss: 3.248202  [600/1425]\n",
      "loss: 3.316826  [700/1425]\n",
      "loss: 3.474220  [800/1425]\n",
      "loss: 3.460438  [900/1425]\n",
      "loss: 3.214560  [1000/1425]\n",
      "loss: 3.262742  [1100/1425]\n",
      "loss: 3.633482  [1200/1425]\n",
      "loss: 3.258060  [1300/1425]\n",
      "loss: 3.523015  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 17.3%, Avg loss: 3.406799 \n",
      "\n",
      "Done!\n",
      "Predicted class probabilities: [[ -9.442416     0.5640284   -3.3503969   -2.5594015   -3.2020812\n",
      "   -3.536734    -2.7881265   -3.0293276   -3.3554473   -5.8422294\n",
      "   -4.5829234   -3.5634563   -5.7898426   -8.27695     -4.7815986\n",
      "   -5.2776546   -6.8606987   -0.07552375 -10.551888    -7.635572\n",
      "   -6.7912765   -6.6354923   -8.004888    -6.8791137   -3.1882925\n",
      "   -3.9695385   -5.7252192   -2.3823228   -4.133982    -6.4234395\n",
      "   -5.773       -5.9971733   -2.4770265   -5.3833365   -6.637038\n",
      "   -7.1578946   -4.44896     -3.8375742    0.6308795   -6.549518\n",
      "  -11.332304    -1.2294575   -3.6088772   -3.6987438   -6.297774\n",
      "   -4.9432874   -0.85734206  -3.038616    -3.561693    -3.764853\n",
      "   -6.049184    -7.53399     -6.225661    -6.986892    -0.98588\n",
      "   -6.592943    -4.1487637   -5.466168    -3.8004665   -3.5509562\n",
      "   -2.8155017   -3.2066183   -6.354743    -5.735204    -3.3596873\n",
      "   -7.3639636   -3.9418132   -8.666491    -7.7384405   -6.86313\n",
      "   -7.626421    -3.960658   -10.235611    -6.4674006   -4.2570496\n",
      "   -4.7393374   -7.09436    -12.227601   -10.496558   -10.533915\n",
      "   -9.619703    -7.985177    -3.9217129   -3.940605    -2.5390258\n",
      "   -4.166377    -4.357951    -0.7807406   -9.477054    -8.365253\n",
      "   -4.839024    -7.3641696   -4.883815    -6.275942    -9.026548\n",
      "   -5.101544    -8.342751   -10.666845    -8.534213    -7.343734\n",
      "   -3.3557062   -8.917128    -7.263719    -4.5836196   -3.990484\n",
      "   -4.2412553   -4.8941946   -3.423103    -4.5961747   -4.634874\n",
      "   -3.531735    -1.0295899   -3.7728868  -12.744748  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:41:05.694.153 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2521907368.py]\n",
      "[ERROR] CORE(439,7f76396f14c0,python):2024-10-14-13:41:05.694.196 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_439/2521907368.py]\n"
     ]
    }
   ],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.dataset as ds\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor, Model,context\n",
    "import mindspore.dataset.vision as vision\n",
    "import mindspore.dataset.transforms as transforms\n",
    "from mindspore.train.callback import LossMonitor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 设置运行环境为 GPU\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")\n",
    "# Step 1: 加载并处理数据集\n",
    "df = pd.read_csv(\"processed_dataset.csv\")\n",
    "\n",
    "# 将 'explicit' 列转换为数值型 (True/False 转换为 1/0)\n",
    "df['explicit'] = df['explicit'].astype(int)\n",
    "\n",
    "# 将类别特征 'track_genre' 做 One-Hot 编码\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "track_genre_encoded = encoder.fit_transform(df[['track_genre']])\n",
    "\n",
    "# 将编码后的数据添加回数据框并移除原始的 'track_genre' 列\n",
    "encoded_columns = encoder.get_feature_names_out(['track_genre'])\n",
    "df_encoded = pd.DataFrame(track_genre_encoded, columns=encoded_columns)\n",
    "df = pd.concat([df.drop(columns=['track_genre']), df_encoded], axis=1)\n",
    "\n",
    "# 对类别特征 'key', 'mode', 'time_signature' 进行 One-Hot 编码\n",
    "categorical_features = ['key', 'mode', 'time_signature']\n",
    "categorical_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "categorical_encoded = categorical_encoder.fit_transform(df[categorical_features])\n",
    "categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=categorical_encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# 合并编码后的数据和数值特征，并移除原始的类别特征\n",
    "df = pd.concat([df.drop(columns=categorical_features).reset_index(drop=True), categorical_encoded_df], axis=1)\n",
    "\n",
    "# 标准化数值特征\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64'])\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features.columns] = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# 将标签转换为 float32 类型\n",
    "df_encoded = df_encoded.astype(np.float32)\n",
    "\n",
    "# 提取特征和标签\n",
    "X = df.drop(columns=encoded_columns).astype(np.float32).values\n",
    "y = df_encoded.values\n",
    "\n",
    "# Step 2: 定义一个简单的 MLP 分类器\n",
    "class MLPClassifier(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # 输入层到第一层隐藏层的全连接层\n",
    "        self.fc1 = nn.Dense(input_size, hidden_sizes[0], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # 第一层隐藏层到第二层隐藏层的全连接层\n",
    "        self.fc2 = nn.Dense(hidden_sizes[0], hidden_sizes[1], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # 第二层隐藏层到输出层的全连接层\n",
    "        self.fc3 = nn.Dense(hidden_sizes[1], output_size, weight_init=\"normal\")\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 数据通过输入层进入第一层隐藏层\n",
    "        x = self.fc1(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu1(x)\n",
    "        # 数据通过第一层隐藏层进入第二层隐藏层\n",
    "        x = self.fc2(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu2(x)\n",
    "        # 数据通过第二层隐藏层进入输出层\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: 创建模型实例\n",
    "input_size = X.shape[1]  # 输入层的特征数量\n",
    "hidden_sizes = [100, 100]  # 隐藏层中的神经元数量\n",
    "output_size = y.shape[1]  # 输出层的神经元数量（类别数量）\n",
    "model = MLPClassifier(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Step 4: 定义超参数、损失函数和优化器\n",
    "epochs = 8\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction='mean')  # 损失函数\n",
    "optimizer = nn.Adam(params=model.trainable_params(), learning_rate=learning_rate)  # 优化器\n",
    "\n",
    "# Step 5: 割分训练集和测试集\n",
    "# 将数据集割分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_dataset = ds.NumpySlicesDataset(data=(X_train, y_train), column_names=['features', 'labels'], shuffle=True).batch(batch_size)\n",
    "test_dataset = ds.NumpySlicesDataset(data=(X_test, y_test), column_names=['features', 'labels'], shuffle=False).batch(batch_size)\n",
    "\n",
    "# Step 6: 定义训练和测试函数\n",
    "def forward_fn(data, label):\n",
    "    logits = model(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss, logits\n",
    "\n",
    "grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "def train_step(data, label):\n",
    "    (loss, _), grads = grad_fn(data, label)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def train_loop(model, dataset, loss_fn):\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "        if batch % 100 == 0:\n",
    "            loss_value = loss.asnumpy()\n",
    "            print(f\"loss: {loss_value:>7f}  [{batch:>3d}/{size:>3d}]\")\n",
    "\n",
    "def test_loop(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label.argmax(1)).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Step 7: 开始训练与测试\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(model, train_dataset, loss_fn)\n",
    "    test_loop(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Step 8: 使用模型进行推理\n",
    "# 创建一个形状为 (1, input_size) 的全为 1 的形状作为输入\n",
    "X_infer = Tensor(np.ones((1, input_size)), ms.float32)\n",
    "# 使用模型进行前向传播，获取输出 logits\n",
    "y_pred = model(X_infer)\n",
    "# 打印预测结果\n",
    "print(f\"Predicted class probabilities: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe15d98-08c2-40cc-8d6b-45e377e6a2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
