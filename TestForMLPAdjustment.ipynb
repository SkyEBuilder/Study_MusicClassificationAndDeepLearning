{
 "cells": [
  {
   "cell_type": "raw",
   "id": "77bcc340-5093-40e8-95b7-eedbc0fff2f6",
   "metadata": {},
   "source": [
    "# 以下测试三层 100 150 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9d6584-3e4d-4a57-8795-6971022ab5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:49:12.108.472 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/3671574363.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:49:12.108.511 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/3671574363.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:49:12.108.526 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/3671574363.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:49:12.108.541 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/3671574363.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.746007  [  0/2850]\n",
      "loss: 4.547727  [100/2850]\n",
      "loss: 4.449991  [200/2850]\n",
      "loss: 4.089506  [300/2850]\n",
      "loss: 4.335276  [400/2850]\n",
      "loss: 4.309237  [500/2850]\n",
      "loss: 3.839746  [600/2850]\n",
      "loss: 3.850352  [700/2850]\n",
      "loss: 3.904968  [800/2850]\n",
      "loss: 3.871197  [900/2850]\n",
      "loss: 4.159403  [1000/2850]\n",
      "loss: 3.878630  [1100/2850]\n",
      "loss: 3.923908  [1200/2850]\n",
      "loss: 3.895950  [1300/2850]\n",
      "loss: 3.775531  [1400/2850]\n",
      "loss: 4.101347  [1500/2850]\n",
      "loss: 3.804650  [1600/2850]\n",
      "loss: 3.912532  [1700/2850]\n",
      "loss: 3.883594  [1800/2850]\n",
      "loss: 3.781496  [1900/2850]\n",
      "loss: 3.562150  [2000/2850]\n",
      "loss: 3.837533  [2100/2850]\n",
      "loss: 3.751155  [2200/2850]\n",
      "loss: 3.876537  [2300/2850]\n",
      "loss: 3.853651  [2400/2850]\n",
      "loss: 3.962660  [2500/2850]\n",
      "loss: 3.743416  [2600/2850]\n",
      "loss: 4.294083  [2700/2850]\n",
      "loss: 3.395361  [2800/2850]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:49:21.574.062 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/3671574363.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:49:21.574.098 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/3671574363.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:49:23.584.541 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/3671574363.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:49:23.584.577 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/3671574363.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      " Accuracy: 12.6%, Avg loss: 3.706143 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.351897  [  0/2850]\n",
      "loss: 3.626379  [100/2850]\n",
      "loss: 3.813746  [200/2850]\n",
      "loss: 4.021282  [300/2850]\n",
      "loss: 3.691102  [400/2850]\n",
      "loss: 3.723602  [500/2850]\n",
      "loss: 4.227628  [600/2850]\n",
      "loss: 3.853584  [700/2850]\n",
      "loss: 3.670685  [800/2850]\n",
      "loss: 3.772881  [900/2850]\n",
      "loss: 3.447707  [1000/2850]\n",
      "loss: 3.802534  [1100/2850]\n",
      "loss: 3.778628  [1200/2850]\n",
      "loss: 3.520260  [1300/2850]\n",
      "loss: 3.652048  [1400/2850]\n",
      "loss: 3.700798  [1500/2850]\n",
      "loss: 3.788813  [1600/2850]\n",
      "loss: 3.941547  [1700/2850]\n",
      "loss: 3.806073  [1800/2850]\n",
      "loss: 3.653757  [1900/2850]\n",
      "loss: 3.935611  [2000/2850]\n",
      "loss: 3.879865  [2100/2850]\n",
      "loss: 3.606355  [2200/2850]\n",
      "loss: 3.574927  [2300/2850]\n",
      "loss: 3.532648  [2400/2850]\n",
      "loss: 3.498623  [2500/2850]\n",
      "loss: 3.702773  [2600/2850]\n",
      "loss: 3.145375  [2700/2850]\n",
      "loss: 3.973557  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 12.9%, Avg loss: 3.651599 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.942368  [  0/2850]\n",
      "loss: 4.104244  [100/2850]\n",
      "loss: 3.677481  [200/2850]\n",
      "loss: 3.872154  [300/2850]\n",
      "loss: 3.436312  [400/2850]\n",
      "loss: 3.557575  [500/2850]\n",
      "loss: 3.739703  [600/2850]\n",
      "loss: 3.494344  [700/2850]\n",
      "loss: 3.835642  [800/2850]\n",
      "loss: 3.819294  [900/2850]\n",
      "loss: 3.680331  [1000/2850]\n",
      "loss: 3.479565  [1100/2850]\n",
      "loss: 3.558122  [1200/2850]\n",
      "loss: 3.482214  [1300/2850]\n",
      "loss: 3.275406  [1400/2850]\n",
      "loss: 3.361051  [1500/2850]\n",
      "loss: 3.568641  [1600/2850]\n",
      "loss: 4.008413  [1700/2850]\n",
      "loss: 3.695074  [1800/2850]\n",
      "loss: 3.648678  [1900/2850]\n",
      "loss: 3.430102  [2000/2850]\n",
      "loss: 3.659340  [2100/2850]\n",
      "loss: 3.429351  [2200/2850]\n",
      "loss: 3.803648  [2300/2850]\n",
      "loss: 3.678791  [2400/2850]\n",
      "loss: 3.577816  [2500/2850]\n",
      "loss: 3.489498  [2600/2850]\n",
      "loss: 3.796382  [2700/2850]\n",
      "loss: 3.714565  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 13.5%, Avg loss: 3.641815 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.654333  [  0/2850]\n",
      "loss: 3.316486  [100/2850]\n",
      "loss: 3.703444  [200/2850]\n",
      "loss: 3.415938  [300/2850]\n",
      "loss: 3.611367  [400/2850]\n",
      "loss: 3.905864  [500/2850]\n",
      "loss: 3.653707  [600/2850]\n",
      "loss: 3.797115  [700/2850]\n",
      "loss: 3.456605  [800/2850]\n",
      "loss: 3.642019  [900/2850]\n",
      "loss: 3.789027  [1000/2850]\n",
      "loss: 3.418287  [1100/2850]\n",
      "loss: 3.301434  [1200/2850]\n",
      "loss: 3.483479  [1300/2850]\n",
      "loss: 3.717373  [1400/2850]\n",
      "loss: 3.666996  [1500/2850]\n",
      "loss: 3.424102  [1600/2850]\n",
      "loss: 3.342327  [1700/2850]\n",
      "loss: 3.931591  [1800/2850]\n",
      "loss: 3.594752  [1900/2850]\n",
      "loss: 4.247646  [2000/2850]\n",
      "loss: 3.465112  [2100/2850]\n",
      "loss: 4.314572  [2200/2850]\n",
      "loss: 3.798741  [2300/2850]\n",
      "loss: 3.687437  [2400/2850]\n",
      "loss: 3.193020  [2500/2850]\n",
      "loss: 3.579103  [2600/2850]\n",
      "loss: 3.623539  [2700/2850]\n",
      "loss: 3.868941  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 13.7%, Avg loss: 3.603201 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.607026  [  0/2850]\n",
      "loss: 3.537844  [100/2850]\n",
      "loss: 3.649738  [200/2850]\n",
      "loss: 3.549036  [300/2850]\n",
      "loss: 3.592327  [400/2850]\n",
      "loss: 3.505278  [500/2850]\n",
      "loss: 3.140969  [600/2850]\n",
      "loss: 4.177926  [700/2850]\n",
      "loss: 3.432942  [800/2850]\n",
      "loss: 3.539375  [900/2850]\n",
      "loss: 3.436017  [1000/2850]\n",
      "loss: 4.065763  [1100/2850]\n",
      "loss: 3.903233  [1200/2850]\n",
      "loss: 3.626863  [1300/2850]\n",
      "loss: 3.449848  [1400/2850]\n",
      "loss: 3.594731  [1500/2850]\n",
      "loss: 3.474707  [1600/2850]\n",
      "loss: 3.329884  [1700/2850]\n",
      "loss: 3.589235  [1800/2850]\n",
      "loss: 3.320474  [1900/2850]\n",
      "loss: 3.644241  [2000/2850]\n",
      "loss: 3.387542  [2100/2850]\n",
      "loss: 3.327486  [2200/2850]\n",
      "loss: 3.282626  [2300/2850]\n",
      "loss: 3.384632  [2400/2850]\n",
      "loss: 3.667018  [2500/2850]\n",
      "loss: 3.700078  [2600/2850]\n",
      "loss: 3.635706  [2700/2850]\n",
      "loss: 3.868228  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 14.4%, Avg loss: 3.589106 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 4.035394  [  0/2850]\n",
      "loss: 3.537373  [100/2850]\n",
      "loss: 3.469987  [200/2850]\n",
      "loss: 3.599072  [300/2850]\n",
      "loss: 3.915887  [400/2850]\n",
      "loss: 3.913280  [500/2850]\n",
      "loss: 3.606050  [600/2850]\n",
      "loss: 3.552430  [700/2850]\n",
      "loss: 3.464784  [800/2850]\n",
      "loss: 3.685050  [900/2850]\n",
      "loss: 3.587596  [1000/2850]\n",
      "loss: 3.798962  [1100/2850]\n",
      "loss: 3.671071  [1200/2850]\n",
      "loss: 3.969092  [1300/2850]\n",
      "loss: 3.505584  [1400/2850]\n",
      "loss: 3.618846  [1500/2850]\n",
      "loss: 3.228225  [1600/2850]\n",
      "loss: 3.497519  [1700/2850]\n",
      "loss: 3.561450  [1800/2850]\n",
      "loss: 3.605727  [1900/2850]\n",
      "loss: 3.642914  [2000/2850]\n",
      "loss: 3.367338  [2100/2850]\n",
      "loss: 3.429953  [2200/2850]\n",
      "loss: 3.361083  [2300/2850]\n",
      "loss: 3.240999  [2400/2850]\n",
      "loss: 3.366806  [2500/2850]\n",
      "loss: 3.709943  [2600/2850]\n",
      "loss: 3.581472  [2700/2850]\n",
      "loss: 3.481003  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 14.8%, Avg loss: 3.569222 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.660413  [  0/2850]\n",
      "loss: 3.811894  [100/2850]\n",
      "loss: 3.674519  [200/2850]\n",
      "loss: 3.049525  [300/2850]\n",
      "loss: 3.784644  [400/2850]\n",
      "loss: 3.577329  [500/2850]\n",
      "loss: 3.586740  [600/2850]\n",
      "loss: 3.741399  [700/2850]\n",
      "loss: 3.773529  [800/2850]\n",
      "loss: 3.869213  [900/2850]\n",
      "loss: 3.643606  [1000/2850]\n",
      "loss: 3.279483  [1100/2850]\n",
      "loss: 3.529692  [1200/2850]\n",
      "loss: 3.677209  [1300/2850]\n",
      "loss: 3.402528  [1400/2850]\n",
      "loss: 3.629539  [1500/2850]\n",
      "loss: 3.826506  [1600/2850]\n",
      "loss: 3.169557  [1700/2850]\n",
      "loss: 3.658379  [1800/2850]\n",
      "loss: 3.224262  [1900/2850]\n",
      "loss: 4.021874  [2000/2850]\n",
      "loss: 3.425164  [2100/2850]\n",
      "loss: 3.636860  [2200/2850]\n",
      "loss: 3.812991  [2300/2850]\n",
      "loss: 3.556161  [2400/2850]\n",
      "loss: 3.799356  [2500/2850]\n",
      "loss: 3.082674  [2600/2850]\n",
      "loss: 3.605191  [2700/2850]\n",
      "loss: 3.225743  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 15.0%, Avg loss: 3.569080 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.642232  [  0/2850]\n",
      "loss: 3.257947  [100/2850]\n",
      "loss: 3.460622  [200/2850]\n",
      "loss: 3.905038  [300/2850]\n",
      "loss: 3.277290  [400/2850]\n",
      "loss: 3.587398  [500/2850]\n",
      "loss: 3.710544  [600/2850]\n",
      "loss: 3.588095  [700/2850]\n",
      "loss: 3.906650  [800/2850]\n",
      "loss: 3.913422  [900/2850]\n",
      "loss: 3.609365  [1000/2850]\n",
      "loss: 3.449103  [1100/2850]\n",
      "loss: 3.472972  [1200/2850]\n",
      "loss: 3.406183  [1300/2850]\n",
      "loss: 3.451952  [1400/2850]\n",
      "loss: 3.702141  [1500/2850]\n",
      "loss: 3.444327  [1600/2850]\n",
      "loss: 3.702705  [1700/2850]\n",
      "loss: 3.370369  [1800/2850]\n",
      "loss: 3.359446  [1900/2850]\n",
      "loss: 3.806746  [2000/2850]\n",
      "loss: 3.784110  [2100/2850]\n",
      "loss: 3.383321  [2200/2850]\n",
      "loss: 3.655609  [2300/2850]\n",
      "loss: 3.719487  [2400/2850]\n",
      "loss: 3.700899  [2500/2850]\n",
      "loss: 3.397373  [2600/2850]\n",
      "loss: 3.188380  [2700/2850]\n",
      "loss: 3.534168  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 14.4%, Avg loss: 3.576856 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.484035  [  0/2850]\n",
      "loss: 3.917525  [100/2850]\n",
      "loss: 3.143008  [200/2850]\n",
      "loss: 3.352154  [300/2850]\n",
      "loss: 3.296474  [400/2850]\n",
      "loss: 3.717067  [500/2850]\n",
      "loss: 3.559142  [600/2850]\n",
      "loss: 3.678605  [700/2850]\n",
      "loss: 3.203992  [800/2850]\n",
      "loss: 4.026821  [900/2850]\n",
      "loss: 3.361760  [1000/2850]\n",
      "loss: 3.547781  [1100/2850]\n",
      "loss: 3.638012  [1200/2850]\n",
      "loss: 3.961644  [1300/2850]\n",
      "loss: 3.469939  [1400/2850]\n",
      "loss: 3.396987  [1500/2850]\n",
      "loss: 2.939385  [1600/2850]\n",
      "loss: 3.492164  [1700/2850]\n",
      "loss: 3.465517  [1800/2850]\n",
      "loss: 2.763508  [1900/2850]\n",
      "loss: 3.489836  [2000/2850]\n",
      "loss: 3.743738  [2100/2850]\n",
      "loss: 3.893325  [2200/2850]\n",
      "loss: 3.114658  [2300/2850]\n",
      "loss: 3.215548  [2400/2850]\n",
      "loss: 3.486946  [2500/2850]\n",
      "loss: 3.655539  [2600/2850]\n",
      "loss: 3.603521  [2700/2850]\n",
      "loss: 2.961949  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 13.8%, Avg loss: 3.589877 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.448371  [  0/2850]\n",
      "loss: 4.135387  [100/2850]\n",
      "loss: 3.553255  [200/2850]\n",
      "loss: 3.619894  [300/2850]\n",
      "loss: 3.826100  [400/2850]\n",
      "loss: 3.677197  [500/2850]\n",
      "loss: 3.651273  [600/2850]\n",
      "loss: 3.633033  [700/2850]\n",
      "loss: 3.863876  [800/2850]\n",
      "loss: 3.676065  [900/2850]\n",
      "loss: 3.362102  [1000/2850]\n",
      "loss: 3.479998  [1100/2850]\n",
      "loss: 3.487324  [1200/2850]\n",
      "loss: 3.578138  [1300/2850]\n",
      "loss: 3.605150  [1400/2850]\n",
      "loss: 2.954795  [1500/2850]\n",
      "loss: 3.276863  [1600/2850]\n",
      "loss: 3.625526  [1700/2850]\n",
      "loss: 3.602337  [1800/2850]\n",
      "loss: 2.938340  [1900/2850]\n",
      "loss: 3.254201  [2000/2850]\n",
      "loss: 3.522350  [2100/2850]\n",
      "loss: 3.605271  [2200/2850]\n",
      "loss: 3.219516  [2300/2850]\n",
      "loss: 3.479981  [2400/2850]\n",
      "loss: 3.583440  [2500/2850]\n",
      "loss: 3.404405  [2600/2850]\n",
      "loss: 3.657534  [2700/2850]\n",
      "loss: 3.208584  [2800/2850]\n",
      "Test: \n",
      " Accuracy: 14.9%, Avg loss: 3.561958 \n",
      "\n",
      "Done!\n",
      "Predicted class probabilities: [[-58.56457  -16.54584  -63.41954  -59.24478  -23.427635 -28.078674\n",
      "  -43.945484 -60.39986  -47.833622 -16.447218 -72.33207  -54.619396\n",
      "  -36.711662 -40.514256 -20.000248 -30.660692 -29.331438 -25.942575\n",
      "   -8.029628 -66.91447  -45.346096 -24.767527 -50.535866 -60.241516\n",
      "  -63.02578  -46.994335 -20.928724 -76.01346  -42.50202  -43.47295\n",
      "  -62.55105  -30.816074 -30.009144 -19.729973 -32.522545 -45.468464\n",
      "  -23.676823 -19.321077 -24.437197 -19.020132 -44.894173 -36.366394\n",
      "  -31.46223  -52.855083 -36.949425 -27.288359 -81.53594  -25.058495\n",
      "  -32.876575 -58.206047 -54.01812  -37.772766 -64.743805 -62.869507\n",
      "  -39.28744  -40.793987 -58.138023 -48.63531  -44.07415  -21.857172\n",
      "  -19.8462   -50.562828 -62.518883 -58.83581  -17.615318 -26.181826\n",
      "  -16.148136 -68.0605   -57.878967 -23.960339 -56.66854  -55.5104\n",
      "  -44.171085 -57.225834 -28.33108  -41.781414 -25.324854 -33.550842\n",
      "  -49.92586  -37.39609  -32.055035 -66.170616 -45.44509  -40.111652\n",
      "  -33.27008  -58.027767 -60.582737 -18.787252 -53.40746  -56.446632\n",
      "  -65.63174  -55.700428 -56.141266 -18.934624 -15.453897 -39.995266\n",
      "  -23.306934 -50.29628  -15.569676 -23.551397 -60.04405  -42.560997\n",
      "  -25.996853 -73.51242  -20.062496 -44.466396 -30.005653 -61.146152\n",
      "  -24.496063 -54.20465  -52.811802 -69.42309  -68.37639  -33.951557]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:50:52.271.234 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/3671574363.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:50:52.271.273 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/3671574363.py]\n"
     ]
    }
   ],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.dataset as ds\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor, Model, context\n",
    "import mindspore.dataset.vision as vision\n",
    "import mindspore.dataset.transforms as transforms\n",
    "from mindspore.train.callback import LossMonitor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 设置使用 GPU\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")\n",
    "\n",
    "# Step 1: 加载并处理数据集\n",
    "df = pd.read_csv(\"processed_dataset.csv\")\n",
    "\n",
    "# 将 'explicit' 列转换为数值型 (True/False 转换为 1/0)\n",
    "df['explicit'] = df['explicit'].astype(int)\n",
    "\n",
    "# 将类别特征 'track_genre' 做 One-Hot 编码\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "track_genre_encoded = encoder.fit_transform(df[['track_genre']])\n",
    "\n",
    "# 将编码后的数据添加回数据框并移除原始的 'track_genre' 列\n",
    "encoded_columns = encoder.get_feature_names_out(['track_genre'])\n",
    "df_encoded = pd.DataFrame(track_genre_encoded, columns=encoded_columns)\n",
    "df = pd.concat([df.drop(columns=['track_genre']), df_encoded], axis=1)\n",
    "\n",
    "# 对类别特征 'key', 'mode', 'time_signature' 进行 One-Hot 编码\n",
    "categorical_features = ['key', 'mode', 'time_signature']\n",
    "categorical_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "categorical_encoded = categorical_encoder.fit_transform(df[categorical_features])\n",
    "categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=categorical_encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# 合并编码后的数据和数值特征，并移除原始的类别特征\n",
    "df = pd.concat([df.drop(columns=categorical_features).reset_index(drop=True), categorical_encoded_df], axis=1)\n",
    "\n",
    "# 标准化数值特征\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64'])\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features.columns] = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# 将标签转换为 float32 类型\n",
    "df_encoded = df_encoded.astype(np.float32)\n",
    "\n",
    "# 提取特征和标签\n",
    "X = df.drop(columns=encoded_columns).astype(np.float32).values\n",
    "y = df_encoded.values\n",
    "\n",
    "# Step 2: 定义一个简单的 MLP 分类器\n",
    "class MLPClassifier(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # 输入层到第一层隐藏层的全连接层\n",
    "        self.fc1 = nn.Dense(input_size, hidden_sizes[0], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # 第一层隐藏层到第二层隐藏层的全连接层\n",
    "        self.fc2 = nn.Dense(hidden_sizes[0], hidden_sizes[1], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # 第二层隐藏层到第三层隐藏层的全连接层\n",
    "        self.fc3 = nn.Dense(hidden_sizes[1], hidden_sizes[2], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # 第三层隐藏层到输出层的全连接层\n",
    "        self.fc4 = nn.Dense(hidden_sizes[2], output_size, weight_init=\"normal\")\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 数据通过输入层进入第一层隐藏层\n",
    "        x = self.fc1(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu1(x)\n",
    "        # 数据通过第一层隐藏层进入第二层隐藏层\n",
    "        x = self.fc2(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu2(x)\n",
    "        # 数据通过第二层隐藏层进入第三层隐藏层\n",
    "        x = self.fc3(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu3(x)\n",
    "        # 数据通过第三层隐藏层进入输出层\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: 创建模型实例\n",
    "input_size = X.shape[1]  # 输入层的特征数量\n",
    "hidden_sizes = [100, 150, 100]  # 隐藏层中的神经元数量\n",
    "output_size = y.shape[1]  # 输出层的神经元数量（类别数量）\n",
    "model = MLPClassifier(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Step 4: 定义超参数、损失函数和优化器\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction='mean')  # 损失函数\n",
    "optimizer = nn.Adam(params=model.trainable_params(), learning_rate=learning_rate)  # 优化器\n",
    "\n",
    "# Step 5: 割分训练集和测试集\n",
    "# 将数据集割分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_dataset = ds.NumpySlicesDataset(data=(X_train, y_train), column_names=['features', 'labels'], shuffle=True).batch(batch_size)\n",
    "test_dataset = ds.NumpySlicesDataset(data=(X_test, y_test), column_names=['features', 'labels'], shuffle=False).batch(batch_size)\n",
    "\n",
    "# Step 6: 定义训练和测试函数\n",
    "def forward_fn(data, label):\n",
    "    logits = model(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss, logits\n",
    "\n",
    "grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "def train_step(data, label):\n",
    "    (loss, _), grads = grad_fn(data, label)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def train_loop(model, dataset, loss_fn):\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "        if batch % 100 == 0:\n",
    "            loss_value = loss.asnumpy()\n",
    "            print(f\"loss: {loss_value:>7f}  [{batch:>3d}/{size:>3d}]\")\n",
    "\n",
    "def test_loop(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label.argmax(1)).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Step 7: 开始训练与测试\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(model, train_dataset, loss_fn)\n",
    "    test_loop(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Step 8: 使用模型进行推理\n",
    "# 创建一个形状为 (1, input_size) 的全为 1 的形状作为输入\n",
    "X_infer = Tensor(np.ones((1, input_size)), ms.float32)\n",
    "# 使用模型进行前向传播，获取输出 logits\n",
    "y_pred = model(X_infer)\n",
    "# 打印预测结果\n",
    "print(f\"Predicted class probabilities: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18502a2-513e-47b3-8aa1-c96429454caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:50:55.526.792 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2178181729.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:50:55.526.825 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2178181729.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:50:55.526.839 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2178181729.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:50:55.526.855 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2178181729.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.751536  [  0/1425]\n",
      "loss: 4.134300  [100/1425]\n",
      "loss: 3.936512  [200/1425]\n",
      "loss: 3.930590  [300/1425]\n",
      "loss: 3.666158  [400/1425]\n",
      "loss: 3.891729  [500/1425]\n",
      "loss: 3.787324  [600/1425]\n",
      "loss: 3.812965  [700/1425]\n",
      "loss: 3.572407  [800/1425]\n",
      "loss: 3.691097  [900/1425]\n",
      "loss: 3.393146  [1000/1425]\n",
      "loss: 3.708950  [1100/1425]\n",
      "loss: 3.594028  [1200/1425]\n",
      "loss: 3.922259  [1300/1425]\n",
      "loss: 3.497707  [1400/1425]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:50:59.823.219 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2178181729.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:50:59.823.253 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2178181729.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:51:01.915.933 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2178181729.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:51:01.915.973 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2178181729.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      " Accuracy: 14.1%, Avg loss: 3.585320 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.553270  [  0/1425]\n",
      "loss: 3.485363  [100/1425]\n",
      "loss: 3.498821  [200/1425]\n",
      "loss: 3.337526  [300/1425]\n",
      "loss: 3.674687  [400/1425]\n",
      "loss: 3.675710  [500/1425]\n",
      "loss: 3.414034  [600/1425]\n",
      "loss: 3.751576  [700/1425]\n",
      "loss: 3.639056  [800/1425]\n",
      "loss: 3.541929  [900/1425]\n",
      "loss: 3.061131  [1000/1425]\n",
      "loss: 3.483802  [1100/1425]\n",
      "loss: 3.263660  [1200/1425]\n",
      "loss: 3.395093  [1300/1425]\n",
      "loss: 3.764843  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 15.3%, Avg loss: 3.536888 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.617478  [  0/1425]\n",
      "loss: 3.618400  [100/1425]\n",
      "loss: 3.120779  [200/1425]\n",
      "loss: 3.831576  [300/1425]\n",
      "loss: 3.643434  [400/1425]\n",
      "loss: 3.723781  [500/1425]\n",
      "loss: 3.495367  [600/1425]\n",
      "loss: 3.531024  [700/1425]\n",
      "loss: 3.496921  [800/1425]\n",
      "loss: 3.438411  [900/1425]\n",
      "loss: 3.450445  [1000/1425]\n",
      "loss: 3.555105  [1100/1425]\n",
      "loss: 3.414358  [1200/1425]\n",
      "loss: 3.228686  [1300/1425]\n",
      "loss: 3.481063  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 15.8%, Avg loss: 3.494280 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.255641  [  0/1425]\n",
      "loss: 3.458669  [100/1425]\n",
      "loss: 3.569348  [200/1425]\n",
      "loss: 4.001049  [300/1425]\n",
      "loss: 3.556239  [400/1425]\n",
      "loss: 3.493538  [500/1425]\n",
      "loss: 3.607118  [600/1425]\n",
      "loss: 3.355726  [700/1425]\n",
      "loss: 3.538222  [800/1425]\n",
      "loss: 3.482231  [900/1425]\n",
      "loss: 3.459058  [1000/1425]\n",
      "loss: 3.267616  [1100/1425]\n",
      "loss: 3.332583  [1200/1425]\n",
      "loss: 3.206783  [1300/1425]\n",
      "loss: 3.503926  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 16.1%, Avg loss: 3.482659 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.262476  [  0/1425]\n",
      "loss: 3.488935  [100/1425]\n",
      "loss: 3.644123  [200/1425]\n",
      "loss: 3.410049  [300/1425]\n",
      "loss: 3.470124  [400/1425]\n",
      "loss: 3.360440  [500/1425]\n",
      "loss: 3.608236  [600/1425]\n",
      "loss: 3.369626  [700/1425]\n",
      "loss: 3.448884  [800/1425]\n",
      "loss: 3.366164  [900/1425]\n",
      "loss: 3.589952  [1000/1425]\n",
      "loss: 3.590115  [1100/1425]\n",
      "loss: 3.595545  [1200/1425]\n",
      "loss: 3.483129  [1300/1425]\n",
      "loss: 3.471406  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 16.6%, Avg loss: 3.451201 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.352861  [  0/1425]\n",
      "loss: 3.391939  [100/1425]\n",
      "loss: 3.658229  [200/1425]\n",
      "loss: 3.274121  [300/1425]\n",
      "loss: 3.307267  [400/1425]\n",
      "loss: 3.384780  [500/1425]\n",
      "loss: 3.326375  [600/1425]\n",
      "loss: 3.232017  [700/1425]\n",
      "loss: 3.332282  [800/1425]\n",
      "loss: 3.231995  [900/1425]\n",
      "loss: 3.654312  [1000/1425]\n",
      "loss: 3.259590  [1100/1425]\n",
      "loss: 3.496883  [1200/1425]\n",
      "loss: 3.179645  [1300/1425]\n",
      "loss: 3.238353  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 16.7%, Avg loss: 3.465392 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.070024  [  0/1425]\n",
      "loss: 3.354598  [100/1425]\n",
      "loss: 3.700639  [200/1425]\n",
      "loss: 3.682741  [300/1425]\n",
      "loss: 3.123570  [400/1425]\n",
      "loss: 3.437933  [500/1425]\n",
      "loss: 3.717264  [600/1425]\n",
      "loss: 3.267117  [700/1425]\n",
      "loss: 3.802958  [800/1425]\n",
      "loss: 3.648880  [900/1425]\n",
      "loss: 3.459625  [1000/1425]\n",
      "loss: 3.354195  [1100/1425]\n",
      "loss: 3.184529  [1200/1425]\n",
      "loss: 3.541915  [1300/1425]\n",
      "loss: 3.518620  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 16.0%, Avg loss: 3.456723 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.501138  [  0/1425]\n",
      "loss: 3.531619  [100/1425]\n",
      "loss: 3.544622  [200/1425]\n",
      "loss: 3.530919  [300/1425]\n",
      "loss: 3.390965  [400/1425]\n",
      "loss: 3.349825  [500/1425]\n",
      "loss: 3.784385  [600/1425]\n",
      "loss: 3.637038  [700/1425]\n",
      "loss: 3.416114  [800/1425]\n",
      "loss: 3.526915  [900/1425]\n",
      "loss: 3.246042  [1000/1425]\n",
      "loss: 3.483261  [1100/1425]\n",
      "loss: 3.358839  [1200/1425]\n",
      "loss: 3.394479  [1300/1425]\n",
      "loss: 3.265116  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 16.3%, Avg loss: 3.468840 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.485628  [  0/1425]\n",
      "loss: 3.428600  [100/1425]\n",
      "loss: 3.118391  [200/1425]\n",
      "loss: 3.312770  [300/1425]\n",
      "loss: 3.422788  [400/1425]\n",
      "loss: 3.467928  [500/1425]\n",
      "loss: 3.416095  [600/1425]\n",
      "loss: 3.595294  [700/1425]\n",
      "loss: 3.724649  [800/1425]\n",
      "loss: 3.451419  [900/1425]\n",
      "loss: 3.344091  [1000/1425]\n",
      "loss: 3.447396  [1100/1425]\n",
      "loss: 3.390346  [1200/1425]\n",
      "loss: 3.743445  [1300/1425]\n",
      "loss: 3.471090  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 16.5%, Avg loss: 3.460601 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.283537  [  0/1425]\n",
      "loss: 3.497483  [100/1425]\n",
      "loss: 3.536198  [200/1425]\n",
      "loss: 3.430281  [300/1425]\n",
      "loss: 3.317366  [400/1425]\n",
      "loss: 3.443349  [500/1425]\n",
      "loss: 3.388912  [600/1425]\n",
      "loss: 3.829619  [700/1425]\n",
      "loss: 3.333718  [800/1425]\n",
      "loss: 3.361361  [900/1425]\n",
      "loss: 3.872042  [1000/1425]\n",
      "loss: 3.560479  [1100/1425]\n",
      "loss: 3.392785  [1200/1425]\n",
      "loss: 3.330140  [1300/1425]\n",
      "loss: 3.519554  [1400/1425]\n",
      "Test: \n",
      " Accuracy: 16.8%, Avg loss: 3.442838 \n",
      "\n",
      "Done!\n",
      "Predicted class probabilities: [[ -3.3423667   -2.7575822   -3.6017613   -2.8801415   -3.102048\n",
      "   -1.600855    -4.057864    -5.0446553   -3.2823083   -4.037554\n",
      "   -4.5156803   -2.368464    -4.8660755   -6.459905    -4.7911863\n",
      "   -1.9699347   -6.981661    -0.914951    -6.727296    -5.6844735\n",
      "   -7.316622    -5.833844    -6.62618     -3.115783    -6.600395\n",
      "   -4.0797744   -4.077612    -4.784549    -2.102196    -1.493424\n",
      "   -4.005614    -1.7132812   -0.14357133  -2.3382542   -3.5110629\n",
      "   -8.567792    -1.0785614   -4.758815    -1.7380226   -2.045761\n",
      "   -5.3958335   -2.6950042  -14.484376    -3.5896027   -4.452354\n",
      "   -4.500127    -4.4890127   -5.016777    -5.569982   -11.312915\n",
      "   -6.2271824   -2.4303753   -7.908888    -6.870405    -1.7860487\n",
      "   -2.3128352   -1.5297899   -2.0568628   -4.3773947   -2.784522\n",
      "   -3.5467548   -6.119771    -4.0924664   -3.6146343   -4.476978\n",
      "   -2.9685726   -5.472097    -3.9895868   -5.2894554   -2.5530496\n",
      "   -2.951276    -5.46595     -4.2922797   -4.5068965   -4.2637167\n",
      "   -5.1158595   -5.2139425   -6.362414    -9.422096    -3.3899999\n",
      "   -3.0449123   -3.919784    -5.0443435   -5.8147426   -4.272045\n",
      "   -3.7463143   -4.4925747   -4.1161275   -6.6117477   -8.733248\n",
      "   -3.9396152   -7.215831    -6.133279    -5.6646748   -2.446078\n",
      "   -7.8890123   -4.955009    -8.460492    -3.1699996   -3.3630636\n",
      "   -5.653879    -9.662996    -3.2285895   -4.0215526   -3.069112\n",
      "   -2.6220407   -3.109145    -2.4976132   -6.977092    -2.7660923\n",
      "   -4.1437793   -1.1346465   -0.8515463   -3.132777  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:06.900.380 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2178181729.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:06.900.426 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2178181729.py]\n"
     ]
    }
   ],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.dataset as ds\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor, Model, context\n",
    "import mindspore.dataset.vision as vision\n",
    "import mindspore.dataset.transforms as transforms\n",
    "from mindspore.train.callback import LossMonitor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 设置使用 GPU\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")\n",
    "\n",
    "# Step 1: 加载并处理数据集\n",
    "df = pd.read_csv(\"processed_dataset.csv\")\n",
    "\n",
    "# 将 'explicit' 列转换为数值型 (True/False 转换为 1/0)\n",
    "df['explicit'] = df['explicit'].astype(int)\n",
    "\n",
    "# 将类别特征 'track_genre' 做 One-Hot 编码\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "track_genre_encoded = encoder.fit_transform(df[['track_genre']])\n",
    "\n",
    "# 将编码后的数据添加回数据框并移除原始的 'track_genre' 列\n",
    "encoded_columns = encoder.get_feature_names_out(['track_genre'])\n",
    "df_encoded = pd.DataFrame(track_genre_encoded, columns=encoded_columns)\n",
    "df = pd.concat([df.drop(columns=['track_genre']), df_encoded], axis=1)\n",
    "\n",
    "# 对类别特征 'key', 'mode', 'time_signature' 进行 One-Hot 编码\n",
    "categorical_features = ['key', 'mode', 'time_signature']\n",
    "categorical_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "categorical_encoded = categorical_encoder.fit_transform(df[categorical_features])\n",
    "categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=categorical_encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# 合并编码后的数据和数值特征，并移除原始的类别特征\n",
    "df = pd.concat([df.drop(columns=categorical_features).reset_index(drop=True), categorical_encoded_df], axis=1)\n",
    "\n",
    "# 标准化数值特征\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64'])\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features.columns] = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# 将标签转换为 float32 类型\n",
    "df_encoded = df_encoded.astype(np.float32)\n",
    "\n",
    "# 提取特征和标签\n",
    "X = df.drop(columns=encoded_columns).astype(np.float32).values\n",
    "y = df_encoded.values\n",
    "\n",
    "# Step 2: 定义一个简单的 MLP 分类器\n",
    "class MLPClassifier(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # 输入层到第一层隐藏层的全连接层\n",
    "        self.fc1 = nn.Dense(input_size, hidden_sizes[0], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # 第一层隐藏层到第二层隐藏层的全连接层\n",
    "        self.fc2 = nn.Dense(hidden_sizes[0], hidden_sizes[1], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # 第二层隐藏层到第三层隐藏层的全连接层\n",
    "        self.fc3 = nn.Dense(hidden_sizes[1], hidden_sizes[2], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # 第三层隐藏层到输出层的全连接层\n",
    "        self.fc4 = nn.Dense(hidden_sizes[2], output_size, weight_init=\"normal\")\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 数据通过输入层进入第一层隐藏层\n",
    "        x = self.fc1(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu1(x)\n",
    "        # 数据通过第一层隐藏层进入第二层隐藏层\n",
    "        x = self.fc2(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu2(x)\n",
    "        # 数据通过第二层隐藏层进入第三层隐藏层\n",
    "        x = self.fc3(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu3(x)\n",
    "        # 数据通过第三层隐藏层进入输出层\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: 创建模型实例\n",
    "input_size = X.shape[1]  # 输入层的特征数量\n",
    "hidden_sizes = [100, 150, 100]  # 隐藏层中的神经元数量\n",
    "output_size = y.shape[1]  # 输出层的神经元数量（类别数量）\n",
    "model = MLPClassifier(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Step 4: 定义超参数、损失函数和优化器\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction='mean')  # 损失函数\n",
    "optimizer = nn.Adam(params=model.trainable_params(), learning_rate=learning_rate)  # 优化器\n",
    "\n",
    "# Step 5: 割分训练集和测试集\n",
    "# 将数据集割分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_dataset = ds.NumpySlicesDataset(data=(X_train, y_train), column_names=['features', 'labels'], shuffle=True).batch(batch_size)\n",
    "test_dataset = ds.NumpySlicesDataset(data=(X_test, y_test), column_names=['features', 'labels'], shuffle=False).batch(batch_size)\n",
    "\n",
    "# Step 6: 定义训练和测试函数\n",
    "def forward_fn(data, label):\n",
    "    logits = model(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss, logits\n",
    "\n",
    "grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "def train_step(data, label):\n",
    "    (loss, _), grads = grad_fn(data, label)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def train_loop(model, dataset, loss_fn):\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "        if batch % 100 == 0:\n",
    "            loss_value = loss.asnumpy()\n",
    "            print(f\"loss: {loss_value:>7f}  [{batch:>3d}/{size:>3d}]\")\n",
    "\n",
    "def test_loop(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label.argmax(1)).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Step 7: 开始训练与测试\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(model, train_dataset, loss_fn)\n",
    "    test_loop(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Step 8: 使用模型进行推理\n",
    "# 创建一个形状为 (1, input_size) 的全为 1 的形状作为输入\n",
    "X_infer = Tensor(np.ones((1, input_size)), ms.float32)\n",
    "# 使用模型进行前向传播，获取输出 logits\n",
    "y_pred = model(X_infer)\n",
    "# 打印预测结果\n",
    "print(f\"Predicted class probabilities: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677e86bc-228e-4f8c-bf81-9f39a091f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:10.167.475 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:10.167.513 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:10.167.528 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:10.167.543 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.740317  [  0/713]\n",
      "loss: 4.079561  [100/713]\n",
      "loss: 3.894694  [200/713]\n",
      "loss: 3.885770  [300/713]\n",
      "loss: 3.838417  [400/713]\n",
      "loss: 3.582614  [500/713]\n",
      "loss: 3.318523  [600/713]\n",
      "loss: 3.677637  [700/713]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:13.864.293 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:13.864.326 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:13.864.341 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:13.864.356 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:13.967.896 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:13.967.930 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:15.551.625 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:52:15.551.664 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: \n",
      " Accuracy: 14.4%, Avg loss: 3.556221 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.610327  [  0/713]\n",
      "loss: 3.572551  [100/713]\n",
      "loss: 3.480569  [200/713]\n",
      "loss: 3.614778  [300/713]\n",
      "loss: 3.466695  [400/713]\n",
      "loss: 3.474248  [500/713]\n",
      "loss: 3.314315  [600/713]\n",
      "loss: 3.444158  [700/713]\n",
      "Test: \n",
      " Accuracy: 15.6%, Avg loss: 3.484475 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.296033  [  0/713]\n",
      "loss: 3.335403  [100/713]\n",
      "loss: 3.521816  [200/713]\n",
      "loss: 3.398899  [300/713]\n",
      "loss: 3.269041  [400/713]\n",
      "loss: 3.241086  [500/713]\n",
      "loss: 3.161129  [600/713]\n",
      "loss: 3.409530  [700/713]\n",
      "Test: \n",
      " Accuracy: 17.1%, Avg loss: 3.435735 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.524062  [  0/713]\n",
      "loss: 3.255773  [100/713]\n",
      "loss: 3.386379  [200/713]\n",
      "loss: 3.249185  [300/713]\n",
      "loss: 3.228599  [400/713]\n",
      "loss: 3.709189  [500/713]\n",
      "loss: 3.585850  [600/713]\n",
      "loss: 3.296715  [700/713]\n",
      "Test: \n",
      " Accuracy: 16.7%, Avg loss: 3.442930 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.218876  [  0/713]\n",
      "loss: 3.330117  [100/713]\n",
      "loss: 3.296747  [200/713]\n",
      "loss: 3.427871  [300/713]\n",
      "loss: 3.250193  [400/713]\n",
      "loss: 3.223458  [500/713]\n",
      "loss: 3.543005  [600/713]\n",
      "loss: 3.498932  [700/713]\n",
      "Test: \n",
      " Accuracy: 16.9%, Avg loss: 3.429714 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.449809  [  0/713]\n",
      "loss: 3.226196  [100/713]\n",
      "loss: 3.184484  [200/713]\n",
      "loss: 3.456176  [300/713]\n",
      "loss: 3.153866  [400/713]\n",
      "loss: 3.270773  [500/713]\n",
      "loss: 3.497723  [600/713]\n",
      "loss: 3.424396  [700/713]\n",
      "Test: \n",
      " Accuracy: 17.4%, Avg loss: 3.398568 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.234215  [  0/713]\n",
      "loss: 3.199498  [100/713]\n",
      "loss: 3.387415  [200/713]\n",
      "loss: 3.156888  [300/713]\n",
      "loss: 3.113658  [400/713]\n",
      "loss: 3.437489  [500/713]\n",
      "loss: 3.331389  [600/713]\n",
      "loss: 3.506954  [700/713]\n",
      "Test: \n",
      " Accuracy: 18.0%, Avg loss: 3.390186 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.278776  [  0/713]\n",
      "loss: 3.381817  [100/713]\n",
      "loss: 3.370965  [200/713]\n",
      "loss: 3.435274  [300/713]\n",
      "loss: 3.351478  [400/713]\n",
      "loss: 3.671863  [500/713]\n",
      "loss: 3.458336  [600/713]\n",
      "loss: 3.537370  [700/713]\n",
      "Test: \n",
      " Accuracy: 17.7%, Avg loss: 3.393656 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.442428  [  0/713]\n",
      "loss: 3.069346  [100/713]\n",
      "loss: 3.469319  [200/713]\n",
      "loss: 3.485188  [300/713]\n",
      "loss: 3.395040  [400/713]\n",
      "loss: 3.609610  [500/713]\n",
      "loss: 3.367282  [600/713]\n",
      "loss: 3.423099  [700/713]\n",
      "Test: \n",
      " Accuracy: 17.9%, Avg loss: 3.389589 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.348327  [  0/713]\n",
      "loss: 3.361569  [100/713]\n",
      "loss: 3.283578  [200/713]\n",
      "loss: 3.303605  [300/713]\n",
      "loss: 3.307680  [400/713]\n",
      "loss: 3.374029  [500/713]\n",
      "loss: 3.438672  [600/713]\n",
      "loss: 3.293290  [700/713]\n",
      "Test: \n",
      " Accuracy: 17.6%, Avg loss: 3.389791 \n",
      "\n",
      "Done!\n",
      "Predicted class probabilities: [[-2.6349533   0.48793274 -1.8503984  -1.4000597  -2.175673   -0.8108133\n",
      "  -5.0702233  -2.7275949  -2.2431226  -1.5243976  -0.7664495  -1.5090163\n",
      "  -2.2235107   0.0711779  -0.9985341  -0.38575798 -5.3779645  -0.09575748\n",
      "  -3.0910912  -2.993082   -2.5075722  -1.5310855  -3.5494027   0.6910459\n",
      "  -0.09814107 -0.06770226 -2.475508   -1.8031187  -0.08525896 -1.4423316\n",
      "  -1.1388137  -1.0363462  -0.082069   -1.8023615  -1.6313722  -3.67925\n",
      "  -0.3981926  -0.9848311  -1.1025991  -0.9273131  -3.350864   -1.5154614\n",
      "  -6.360441   -0.30437398 -2.9796758  -2.8144217  -0.68343294 -1.8039212\n",
      "  -1.1150544  -0.88968515 -2.8461769  -1.0503883  -2.5967226  -1.1169229\n",
      "  -0.1351849  -0.81969666 -0.6218823  -0.9324497  -0.55053765 -3.8907058\n",
      "  -0.9840178  -3.3980558  -2.7873633  -2.5596502  -2.3693142  -1.7159247\n",
      "   0.2417931  -2.1806452  -2.5291514  -0.11742616 -2.8358002  -2.4906504\n",
      "  -2.8033261  -0.33947384 -1.4538689  -3.1143997  -4.9400644  -3.2611568\n",
      "  -0.9288554  -3.8495836  -2.093972   -1.3920648  -1.6317     -0.6696404\n",
      "  -1.0959415  -1.8761325  -1.3768094  -1.3353062  -2.5652485  -2.3702226\n",
      "  -1.7511469  -2.663246   -1.2327589  -5.6572227  -1.20153    -2.8040776\n",
      "  -1.0771012  -3.2849486  -2.8922372  -2.9064033  -0.07010424 -6.1520405\n",
      "  -3.3629034  -1.2152491  -0.96718806  0.45300865 -1.4869658  -0.6261661\n",
      "  -5.1912494  -0.46724483  0.1970751   0.31236535 -0.19895542 -4.5750093 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:53:01.749.234 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n",
      "[ERROR] CORE(1760,7f2fed8464c0,python):2024-10-14-13:53:01.749.277 [mindspore/core/utils/file_utils.cc:253] GetRealPath] Get realpath failed, path[/tmp/ipykernel_1760/2105147425.py]\n"
     ]
    }
   ],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.dataset as ds\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor, Model, context\n",
    "import mindspore.dataset.vision as vision\n",
    "import mindspore.dataset.transforms as transforms\n",
    "from mindspore.train.callback import LossMonitor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 设置使用 GPU\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\")\n",
    "\n",
    "# Step 1: 加载并处理数据集\n",
    "df = pd.read_csv(\"processed_dataset.csv\")\n",
    "\n",
    "# 将 'explicit' 列转换为数值型 (True/False 转换为 1/0)\n",
    "df['explicit'] = df['explicit'].astype(int)\n",
    "\n",
    "# 将类别特征 'track_genre' 做 One-Hot 编码\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "track_genre_encoded = encoder.fit_transform(df[['track_genre']])\n",
    "\n",
    "# 将编码后的数据添加回数据框并移除原始的 'track_genre' 列\n",
    "encoded_columns = encoder.get_feature_names_out(['track_genre'])\n",
    "df_encoded = pd.DataFrame(track_genre_encoded, columns=encoded_columns)\n",
    "df = pd.concat([df.drop(columns=['track_genre']), df_encoded], axis=1)\n",
    "\n",
    "# 对类别特征 'key', 'mode', 'time_signature' 进行 One-Hot 编码\n",
    "categorical_features = ['key', 'mode', 'time_signature']\n",
    "categorical_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "categorical_encoded = categorical_encoder.fit_transform(df[categorical_features])\n",
    "categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=categorical_encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# 合并编码后的数据和数值特征，并移除原始的类别特征\n",
    "df = pd.concat([df.drop(columns=categorical_features).reset_index(drop=True), categorical_encoded_df], axis=1)\n",
    "\n",
    "# 标准化数值特征\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64'])\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features.columns] = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# 将标签转换为 float32 类型\n",
    "df_encoded = df_encoded.astype(np.float32)\n",
    "\n",
    "# 提取特征和标签\n",
    "X = df.drop(columns=encoded_columns).astype(np.float32).values\n",
    "y = df_encoded.values\n",
    "\n",
    "# Step 2: 定义一个简单的 MLP 分类器\n",
    "class MLPClassifier(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # 输入层到第一层隐藏层的全连接层\n",
    "        self.fc1 = nn.Dense(input_size, hidden_sizes[0], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # 第一层隐藏层到第二层隐藏层的全连接层\n",
    "        self.fc2 = nn.Dense(hidden_sizes[0], hidden_sizes[1], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # 第二层隐藏层到第三层隐藏层的全连接层\n",
    "        self.fc3 = nn.Dense(hidden_sizes[1], hidden_sizes[2], weight_init=\"normal\")\n",
    "        # 激活函数 ReLU\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # 第三层隐藏层到输出层的全连接层\n",
    "        self.fc4 = nn.Dense(hidden_sizes[2], output_size, weight_init=\"normal\")\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 数据通过输入层进入第一层隐藏层\n",
    "        x = self.fc1(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu1(x)\n",
    "        # 数据通过第一层隐藏层进入第二层隐藏层\n",
    "        x = self.fc2(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu2(x)\n",
    "        # 数据通过第二层隐藏层进入第三层隐藏层\n",
    "        x = self.fc3(x)\n",
    "        # 使用 ReLU 激活函数\n",
    "        x = self.relu3(x)\n",
    "        # 数据通过第三层隐藏层进入输出层\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: 创建模型实例\n",
    "input_size = X.shape[1]  # 输入层的特征数量\n",
    "hidden_sizes = [100, 150, 100]  # 隐藏层中的神经元数量\n",
    "output_size = y.shape[1]  # 输出层的神经元数量（类别数量）\n",
    "model = MLPClassifier(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Step 4: 定义超参数、损失函数和优化器\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction='mean')  # 损失函数\n",
    "optimizer = nn.Adam(params=model.trainable_params(), learning_rate=learning_rate)  # 优化器\n",
    "\n",
    "# Step 5: 割分训练集和测试集\n",
    "# 将数据集割分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_dataset = ds.NumpySlicesDataset(data=(X_train, y_train), column_names=['features', 'labels'], shuffle=True).batch(batch_size)\n",
    "test_dataset = ds.NumpySlicesDataset(data=(X_test, y_test), column_names=['features', 'labels'], shuffle=False).batch(batch_size)\n",
    "\n",
    "# Step 6: 定义训练和测试函数\n",
    "def forward_fn(data, label):\n",
    "    logits = model(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss, logits\n",
    "\n",
    "grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "def train_step(data, label):\n",
    "    (loss, _), grads = grad_fn(data, label)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def train_loop(model, dataset, loss_fn):\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "        if batch % 100 == 0:\n",
    "            loss_value = loss.asnumpy()\n",
    "            print(f\"loss: {loss_value:>7f}  [{batch:>3d}/{size:>3d}]\")\n",
    "\n",
    "def test_loop(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label.argmax(1)).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# Step 7: 开始训练与测试\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(model, train_dataset, loss_fn)\n",
    "    test_loop(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Step 8: 使用模型进行推理\n",
    "# 创建一个形状为 (1, input_size) 的全为 1 的形状作为输入\n",
    "X_infer = Tensor(np.ones((1, input_size)), ms.float32)\n",
    "# 使用模型进行前向传播，获取输出 logits\n",
    "y_pred = model(X_infer)\n",
    "# 打印预测结果\n",
    "print(f\"Predicted class probabilities: {y_pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
